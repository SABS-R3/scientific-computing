[
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_0_introduction/",
	"title": "Course introduction",
	"tags": [],
	"description": "",
	"content": "Chapter 0 Course introduction "
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_1_linear_algebra/01-matrix-form-of-equations/",
	"title": "Matrix form of equations",
	"tags": [],
	"description": "",
	"content": "Systems of linear equations Linear algebra is largely concerned with representing, manipulating and solving large systems of linear equations. Consider the following 2 linear equations:\n\\[ \\begin{aligned} a_1x+b_1y \u0026= c_1, \\quad (1)\\\\ a_2x+b_2y \u0026= c_2, \\quad (2) \\end{aligned} \\]\nwhere the values $\\;x\\;$ and $\\;y\\;$ are to be found, and $\\;a_1, \\;b_1, \\;a_2, \\;b_2, \\;c_1\\;$ and $\\;c_2\\;$ are given constants. We know that we can use linear combinations of these two equations to solve this sytem for $\\;x\\;$ and $\\;y\\;$, like so:\n\\[ \\begin{aligned} (1) \\times b_2:~~~~~~~~~~~~~~~ b_2a_1x+b_2b_1y \u0026=\u0026 b_2c_1, \\quad (3)\\\\ (2) \\times b_1:~~~~~~~~~~~~~~~ b_1a_2x+b_1b_2y \u0026=\u0026 b_1c_2, \\quad (4)\\\\ (3) - (4):~~~~~~~~~~~~~~~ b_2a_1x-b_1a_2x \u0026=\u0026 b_2c_1-b_1c_2. \\end{aligned} \\]\nThe Matrix We can also write these linear equations using a matrix. Matrices are structures that allow us to more easily manipulate linear systems. While not particularly useful for just 2 equations, using a matrix representation allows us to generalise to, say $N$ equations and $M$ unknowns, or to solve large systems of equations on a computer.\nConsider the original system:\n\\[ \\begin{aligned} a_1x+b_1y \u0026= c_1, \\\\ a_2x+b_2y \u0026= c_2. \\end{aligned} \\]\nWe rewrite this, in the form of a matrix as:\n\\[ \\left(\\begin{matrix}a_1\u0026b_1\\\\ a_2\u0026b_2\\end{matrix}\\right) \\left(\\begin{matrix}x\\\\y\\end{matrix}\\right) =\\left(\\begin{matrix}c_1\\\\ c_2 \\end{matrix}\\right). \\]\nThink about how this form relates to the original linear system.\nGeometry of linear equations Consider the following system of equations\n\\[ \\begin{aligned} x + -2y \u0026= -1, \\\\ -x + 3y \u0026= 3. \\end{aligned} \\]\nThat is,\n\\[A = \\left(\\begin{matrix} 1 \u0026 -2 \\\\ -1 \u0026 3\\end{matrix}\\right)\\]\nPlotting these two linear equations on a graph shows graphically the solution to this equation given by\n\\[\\left(\\begin{matrix} x \\\\ y \\end{matrix}\\right) = A^{-1} \\left(\\begin{matrix} -1 \\\\ 3 \\end{matrix}\\right) = \\left(\\begin{matrix} -1 \\\\ 3 \\end{matrix}\\right)\\]\n\nNow lets consider two different system of equations represented by the matrix:\n\\[ A = \\left(\\begin{matrix} 1 \u0026 -2 \\\\ -1 \u0026 2\\end{matrix}\\right) \\]\n\\[ \\begin{aligned} x + -2y \u0026= -1, \\\\ -x + 2y \u0026= 3. \\end{aligned} \\]\nand\n\\[ \\begin{aligned} x + -2y \u0026= -1, \\\\ -x + 2y \u0026= 1. \\end{aligned} \\]\n\nThe first gives the plot on the left, while the second, which has a different vector of constants on the RHS, gives the plot on the right. You can see that depending on the constants, the system of equations can have an infinite number of solutions, or no solutions at all.\nThe matrix $A$ in this case is singular, and therefore does not have an inverse. Looking at the equations again, you can see that the two rows of the matrix $A$ are multiples of the other, and thus there is only one independent row. That is, the rank of the matrix is one.\nSingular matrices The rank of an $\\;n\\,\\times\\,n\\;$ matrix $\\;A\\;$ is the number of linearly independent rows in $\\;A\\;$ (rows not combinations of other rows).\nWhen $\\;\\text{rank}(A) \u0026lt; n\\;$ then\n The matrix is said to be 'rank deficient' The system $\\;A\\textbf{x} = \\textbf{b}\\;$ has fewer equations than unknowns The matrix is said to be singular The matrix is said to be underdetermined $A\\;$ has no inverse The determinant of $\\;A\\;$ is 0 The equation $\\;A\\textbf{u} = \\textbf{0}\\;$ has non-trivial solutions ($\\textbf{u} \\neq \\textbf{0}$)  \u0026quot;## Null space\\n\u0026quot;,\n\u0026#34;When a matrix is singular we can find non-trivial solutions to $\\\\;A\\\\textbf{u} = \\\\textbf{0}$.\\n\u0026#34;, \u0026#34;\\n\u0026#34;, \u0026#34;These are vectors which form a *null space* for $\\\\;A$.\\n\u0026#34;, \u0026#34;\\n\u0026#34;, \u0026#34;These vectors make no difference to the effect that $A$ is having:\\n\u0026#34;, \u0026#34;\\n\u0026#34;, \u0026#34;$$\\n\u0026#34;, \u0026#34; A(\\\\textbf{x} + \\\\textbf{u}) = A\\\\textbf{x} + A\\\\textbf{u} =\\n\u0026#34;, \u0026#34; A\\\\textbf{x} + \\\\textbf{0} = A\\\\textbf{x}.\\n\u0026#34;, \u0026#34;$$ \u0026#34; The determinant One way of solving a system of equations represented by $A x = b$ is to calculate the inverse of A, giving the solution as $x = A^{-1} b$. This can be done by calculating what is known as the determinant of $A$.\nIf\n\\[A = \\left(\\begin{matrix} p \u0026 q \\\\ r \u0026 s\\end{matrix}\\right)\\]\nthen the determinant of A is:\n\\[|A| = ps-qr\\]\nThe inverse of $A$ can be found using the determinant:\n\\[ A^{-1} = \\frac{1}{ps-qr} \\left(\\begin{matrix} s \u0026 -q \\\\ -r \u0026 p\\end{matrix}\\right) \\]\nCalculating the inverse of a matrix using its determinant can be very costly for larger matrices, therefore other algorithms are used (e.g. Gaussian Elimination, which is introduced in the next section)\nIf $|A| = 0$, A is said to be singular (have no inverse). Graphically, this is represented by the parallel or non-intersecting lines in the figure above.\nUsing Python to calculate the inverse To find $A^{-1}$ for\n\\[ A = \\left(\\begin{matrix}3\u00260\u00262\\\\ 3\u00260\u0026-3\\\\ 0\u00261\u0026-1\\end{matrix}\\right) \\]\nyou can using numpy like so:\nA = np.array([[3, 0, 2], [3, 0, -3], [0, 1, 1]]) np.linalg.inv(A) Output:\narray([[ 0.2 , 0.13333333, 0. ], [-0.2 , 0.2 , 1. ], [ 0.2 , -0.2 , -0. ]]) It doesn't always work. Consider the following system\n\\[ A = \\left(\\begin{matrix}1\u00261\u00261\\\\ 2\u00264\u00262\\\\ 7\u002610\u00267\\end{matrix}\\right) \\]\nA = np.array([[1, 1, 1], [2, 5, 2], [7, 10, 7]]) np.linalg.inv(A) Output:\nTraceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; File \u0026#34;\u0026lt;__array_function__ internals\u0026gt;\u0026#34;, line 6, in inv File \u0026#34;/home/mrobins/git/scientific-computing/env/lib/python3.6/site-packages/numpy/linalg/linalg.py\u0026#34;, line 546, in inv ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj) File \u0026#34;/home/mrobins/git/scientific-computing/env/lib/python3.6/site-packages/numpy/linalg/linalg.py\u0026#34;, line 88, in _raise_linalgerror_singular raise LinAlgError(\u0026#34;Singular matrix\u0026#34;) numpy.linalg.LinAlgError: Singular matrix Other Reading Linear algebra by Ward Cheney Linear algebra and its applications by David C. Lay.\nStrang, G. (2016). Introduction to linear algebra (Fifth ed.). Wellesley.\nLinear algebra and its applications by Gilbert Strang\nlots of supplimentary material for this last two via MIT course page here: https://github.com/mitmath/1806/blob/master/summaries.md\n LA from an ODE perspective Kapitula, T. (2015). Ordinary Differential Equations and Linear Algebra. Society for Industrial and Applied Mathematics.  Problems Question  Describe the intersection of the three planes $u+v+w+z = 6$, $u+w+z = 4$ and $u+w = 2$ (all in four-dimensional space). Is it a line or a point or a fourth equation that leaves us with no solution. an empty set? What is the intersection if the fourth plane $u = −1$ is included? Find a fourth equation that leaves us with no solution.  \n\u0026lt;summary\u0026gt;Expand for solution\u0026lt;/summary\u0026gt; \u0026lt;div class=\u0026#34;notices solution\u0026#34; \u0026gt; Solution The intersection of the 3 planes is the 1d line $u + w = 2$, $v=2$ and $z=2$. Introducing a fourth equation that does not intersect this line (e.g. $u + w = 3$) leaves us with no solutions.\n\n\n Question Sketch or plot in Python these three lines and decide if the equations are solvable: 3 by 2 system $x + 2y = 2$, $x − y = 2$, and $y = 1$. What happens if all right-hand sides are zero? Is there any nonzero choice of right- hand sides that allows the three lines to intersect at the same point?  \n\u0026lt;summary\u0026gt;Expand for solution\u0026lt;/summary\u0026gt; \u0026lt;div class=\u0026#34;notices solution\u0026#34; \u0026gt; Solution import numpy as np import matplotlib.pylab as plt\nx = np.linspace(-1, 4, 100)\ndef plot_lines(b1, b2, b3):\ny1 \u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;=\u0026lt;/span\u0026gt; (b1 \u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;-\u0026lt;/span\u0026gt; x) \u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;/\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;2\u0026lt;/span\u0026gt; y2 \u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;=\u0026lt;/span\u0026gt; x \u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;-\u0026lt;/span\u0026gt; b2 y3 \u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;=\u0026lt;/span\u0026gt; b3 \u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;*\u0026lt;/span\u0026gt; np\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;.\u0026lt;/span\u0026gt;ones_like(x) plt\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;.\u0026lt;/span\u0026gt;plot(x, y1) plt\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;.\u0026lt;/span\u0026gt;plot(x, y2) plt\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;.\u0026lt;/span\u0026gt;plot(x, y3) plt\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;.\u0026lt;/span\u0026gt;show() plot_lines(2, 2, 1) plot_lines(0, 0, 0) plot_lines(2, -1, 1) \n\n Question Write a Python function that takes in a $3 \\times 3$ upper triangular matrix $A$ represented as an ndarray, and a rhs vector $b$, and solves the equation $A x = b$. i.e. the function will solve the following triangular system for $x = (x_1, x_2, x_3)$:  \\[ \\begin{aligned} A_{11} x_1 + A_{12} x_2 + A_{13} x_3 \u0026= b_1, \\\\ A_{22} x_2 + A_{23} x_3 \u0026= b_2, \\\\ A_{33} x_3 \u0026= b_3 \\end{aligned} \\]\nGeneralise this function to a $n \\times n$ triangular matrix input.\n\n\u0026lt;summary\u0026gt;Expand for solution\u0026lt;/summary\u0026gt; \u0026lt;div class=\u0026#34;notices solution\u0026#34; \u0026gt; Solution def solve_triangular(A, b):\nn \u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;=\u0026lt;/span\u0026gt; len(b) x \u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;=\u0026lt;/span\u0026gt; np\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;.\u0026lt;/span\u0026gt;empty_like(b) \u0026lt;span style=\u0026#34;color:#66d9ef\u0026#34;\u0026gt;for\u0026lt;/span\u0026gt; i \u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;in\u0026lt;/span\u0026gt; range(n\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;-\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;1\u0026lt;/span\u0026gt;, \u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;-\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;1\u0026lt;/span\u0026gt;, \u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;-\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;1\u0026lt;/span\u0026gt;): x[i] \u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;=\u0026lt;/span\u0026gt; b[i] \u0026lt;span style=\u0026#34;color:#66d9ef\u0026#34;\u0026gt;for\u0026lt;/span\u0026gt; j \u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;in\u0026lt;/span\u0026gt; range(n\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;-\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;1\u0026lt;/span\u0026gt;, i, \u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;-\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;1\u0026lt;/span\u0026gt;): x[i] \u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;-=\u0026lt;/span\u0026gt; A[i, j] \u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;*\u0026lt;/span\u0026gt; x[j] x[i] \u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;/=\u0026lt;/span\u0026gt; A[i, i] \u0026lt;span style=\u0026#34;color:#66d9ef\u0026#34;\u0026gt;return\u0026lt;/span\u0026gt; x def random_upper_triangular(n):\nR \u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;=\u0026lt;/span\u0026gt; np\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;.\u0026lt;/span\u0026gt;random\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;.\u0026lt;/span\u0026gt;rand(n, n) A \u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;=\u0026lt;/span\u0026gt; np\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;.\u0026lt;/span\u0026gt;zeros((n, n)) triu \u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;=\u0026lt;/span\u0026gt; np\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;.\u0026lt;/span\u0026gt;triu_indices(n) A[triu] \u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;=\u0026lt;/span\u0026gt; R[triu] \u0026lt;span style=\u0026#34;color:#66d9ef\u0026#34;\u0026gt;return\u0026lt;/span\u0026gt; A As = [\nnp\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;.\u0026lt;/span\u0026gt;array([[\u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;1\u0026lt;/span\u0026gt;, \u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;0\u0026lt;/span\u0026gt;, \u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;0\u0026lt;/span\u0026gt;], [\u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;0\u0026lt;/span\u0026gt;, \u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;1\u0026lt;/span\u0026gt;, \u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;0\u0026lt;/span\u0026gt;], [\u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;0\u0026lt;/span\u0026gt;, \u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;0\u0026lt;/span\u0026gt;, \u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;1\u0026lt;/span\u0026gt;]]), random_upper_triangular(\u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;3\u0026lt;/span\u0026gt;), random_upper_triangular(\u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;4\u0026lt;/span\u0026gt;), random_upper_triangular(\u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;5\u0026lt;/span\u0026gt;), random_upper_triangular(\u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;6\u0026lt;/span\u0026gt;), ]\nbs = [\nnp\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;.\u0026lt;/span\u0026gt;array([\u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;1\u0026lt;/span\u0026gt;, \u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;2\u0026lt;/span\u0026gt;, \u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;3\u0026lt;/span\u0026gt;]), np\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;.\u0026lt;/span\u0026gt;random\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;.\u0026lt;/span\u0026gt;rand(\u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;3\u0026lt;/span\u0026gt;), np\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;.\u0026lt;/span\u0026gt;random\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;.\u0026lt;/span\u0026gt;rand(\u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;4\u0026lt;/span\u0026gt;), np\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;.\u0026lt;/span\u0026gt;random\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;.\u0026lt;/span\u0026gt;rand(\u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;5\u0026lt;/span\u0026gt;), np\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;.\u0026lt;/span\u0026gt;random\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;.\u0026lt;/span\u0026gt;rand(\u0026lt;span style=\u0026#34;color:#ae81ff\u0026#34;\u0026gt;6\u0026lt;/span\u0026gt;), ]\nfor A, b in zip(As, bs):\nx_scipy \u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;=\u0026lt;/span\u0026gt; scipy\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;.\u0026lt;/span\u0026gt;linalg\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;.\u0026lt;/span\u0026gt;solve_triangular(A, b) x_mine \u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;=\u0026lt;/span\u0026gt; solve_triangular(A, b) np\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;.\u0026lt;/span\u0026gt;testing\u0026lt;span style=\u0026#34;color:#f92672\u0026#34;\u0026gt;.\u0026lt;/span\u0026gt;assert_almost_equal(x_scipy, x_mine)\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\u0026lt;/div\u0026gt; \n\n "
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_4_optimisation/01-nonlinear-optimisation/",
	"title": "Nonlinear Optimisation",
	"tags": [],
	"description": "",
	"content": "Mathematical formulation Optimisation aims to find the minimum (or equivilently the maximum) of some objective, or loss function $f$, given a set of $n$ parameters $\\theta$\n\\[ \\min_{\\theta \\in \\mathcal{R}^n} f(\\theta) \\]\nWe might also have a set of constraints, for example a parameter might be required to be non-negative (e.g. a concentration or population number). These are often written as a set of equality $\\mathcal{E}$ and inequality $\\mathcal{I}$ constraints\n\\[ \\min_{\\theta \\in \\mathcal{R}^n} f(\\theta) \\text{ subject to } \\begin{cases} c_i(x) = 0, \u0026 i \\in \\mathcal{E} \\\\ c_i(x) \\ge 0, \u0026 i \\in \\mathcal{I} \\end{cases} \\]\nOr these might simply be defined as bounds in parameter space that restrict the minimisation to a given domain $\\Omega \\in \\mathcal{R}^n$\n\\[ \\min_{\\theta \\in \\Omega} f(\\theta) \\]\nUseful terms Modelling is the process of defining the objective function $f$, the parameters of interest $\\theta$, and the constraints. The algorithms for performing the minimisation fall under the field of optimisation. Sub-fields of this are concerned with the minimisation of discrete function, often called integer programming. Confusingly, it is common to see the terms \u0026quot;optimisation\u0026quot; and \u0026quot;programming\u0026quot; used interchangably, as thie latter term was coined before the 1940s, and does not refer to computer software programming at all.\nIf the function $f$ is linear, then there are specific algorithms for this class of problem that fall under the topic of linear programming, or linear optimisation. The more general problem of a non-linear $f$ is termed non-linear programming, or non-linear optimisation. If a set of equality and/or inequality constraints are needed then algorithms that deal with these fall under the topic of constrained optimisation.\nAn important distinction when looking at optimisation problems is the notion of global versus local optimisation. The latter finds a point in parameter space $\\theta_m$ that has a function value $f(\\theta_m)$ greater than the surrounding points, but might not neccessarily by the global minimum. These algorithms are often initialised to a point that is near to the minima of interest. The more general problem of global optimisation is significantly more difficult as it requires that the optimisation be robust to finding and rejecting such local minima. For a function that is convex, then local and global minimisation are the same, which is very advantagous since local minimisation algorithms are often both faster and often have more guarentees of convergence. The function $f$ is a convex function if its domain $\\Omega$ is a convex set, and for any two points $\\theta_x$ and $\\theta_y$:\n\\[ f(\\alpha \\theta_x + (1 - \\alpha) \\theta_y ) \\le \\alpha f(\\theta_x) + (1 - \\alpha) f(\\theta_y), \\text{ for all } \\alpha \\in [0, 1]. \\]\nThe term convex programming is used to describe the case of contrained optimisation where $f$ is convex, the equality constraints are linear and the inequality contraints are concave.\nNon-linear optimisation and Root-Finding Non-linear optimisation is closely related to finding the roots, or zeros, of a function. This can be seen easily by considering the fact that at each local minima or maxima of the function the value of the gradient of $f$ is zero, i.e. $\\nabla f = 0$. Therefore finding a local minima or maxima of $f$ corresponds to finding the zeros of the function $g = \\nabla f$.\nOther reading  Numerical optimization by Nocedal, Jorge; Wright, Stephen J., 1960- Bazaraa, Sherali, and Shetty, Nonlinear Optimization, 2/e, Wiley Griva, Nash, and Sofer, Linear and Nonlinear Optimization, 2/e, SIAM Press Luenberger, Linear and Nonlinear Programming, 3/e, Springer Bertsekas, Nonlinear Programming, Athena Ruszczynski, Nonlinear Optimization, Princeton University Press Broyden, C. G. (1972). \u0026quot;Quasi-Newton Methods\u0026quot;. In Murray, W. (ed.). Numerical Methods for Unconstrained Optimization. London: Academic Press. pp. 87–106. ISBN 0-12-512250-0.  "
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/01-sparse-matrices/",
	"title": "Sparse matrices",
	"tags": [],
	"description": "",
	"content": "Why sparse matrices Taking advantage of any special structure in the matrix of interest is always of great importance when designing a linear algebra algorithm/solver. Thus far we have discussed special structures such as symmetric or positive definite matrices, but one of the most common matrix structures in scientific computing is that of a sparse matrix, or a matrix containing many zero elements. Since zeros can be ignored in many computations, for example multiplication or addition, a sparse matrix will specify a special data structure so that only the non-zero elements of the matrix are actually stored and used in computation. Note also that a sparse matrix can itself be symmetric or positive definite, and it is often neccessary to take all these properties into account when designing your algorithm.\nThe most obvious benifit of a sparse matrix is low memory usage. A dense matrix needs to store all the elements of a matrix of size $n$, and thus the memory requirements scale as $\\mathcal{O}(n^2)$. For example, the following show the memory requirements of a matrix of double precision numbers (taken from the excellent scipy-lectures)\n\nA sparse matrix only stores non-zero elements, and in many different applications this represents a huge memory saving as matrices are often very sparse, holding only a few non-zero elements. Some typical applications are:\n solution of partial differential equations (PDEs), such as the finite difference method illustrated below applications of graphs or networks (e.g. electrical networks, website links), where non-zero elements of the matrix represent edges between nodes  Note that while a sparse matrix has obvious benifits in terms of matrix multiplication, where the zero elements can simply be ignored, direct solver algorithms such as $LU$ decodecomposition for the problem $Ax = b$, where $A$ is sparse, need considerably more thought as the zeros in $A$ can have propogating effects, and there is no guarentee that the decomposition of a $A$ or its inverse will be itself sparse, there can be a significant amount of what is known as fill-in. This fact motivates a separate class of iterative (as opposed to direct) solvers that only rely on the matrix multiplication of $A$ with a vector, ignoring the internal sparsity structure of $A$ and only taking advantage of the increased speed of the matrix multiplication itself. These iterative solvers will be covered in the following chapter, but in this chapter we will focus on the practical requirements of constructing and using sparse matrices using the scipy.sparse library.\nAn example sparse format The COOrdinate format (COO) is also known as the \u0026quot;ijv\u0026quot; or \u0026quot;triplet\u0026quot; format, and stores the non-zero elements in three arrays, row, col, and data. The data[i] value is the non-zero entry in row row[i] and column col[i] of the matrix. The advantages of this format are:\n fast format for contructing sparse matrices fast conversions to/from the CSR and CSC formats fast matrix-vector multiplication fast elementwise operations (e.g. multiply each element by 2 is just data * 2)  However, slicing using this format is difficult.\nHere are some examples of the COO matrix format using scipy.sparse.coo_matrix. Again, these have been taken from scipy-lectures, which is an excellent resource and contains examples of the other sparse matrix formats implemented in Scipy.\ncreate empty COO matrix: mtx = sparse.coo_matrix((3, 4), dtype=np.int8) mtx.todense() Output:\nmatrix([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], dtype=int8) create using (data, ij) tuple: row = np.array([0, 3, 1, 0]) col = np.array([0, 3, 1, 2]) data = np.array([4, 5, 7, 9]) mtx = sparse.coo_matrix((data, (row, col)), shape=(4, 4)) mtx mtx.todense() Output:\n\u0026gt;\u0026gt;\u0026gt; mtx \u0026lt;4x4 sparse matrix of type \u0026#39;\u0026lt;class \u0026#39;numpy.int64\u0026#39;\u0026gt;\u0026#39; with 4 stored elements in COOrdinate format\u0026gt; \u0026gt;\u0026gt;\u0026gt; mtx.todense() matrix([[4, 0, 9, 0], [0, 7, 0, 0], [0, 0, 0, 0], [0, 0, 0, 5]]) duplicates entries are summed together: row = np.array([0, 0, 1, 3, 1, 0, 0]) col = np.array([0, 2, 1, 3, 1, 0, 0]) data = np.array([1, 1, 1, 1, 1, 1, 1]) mtx = sparse.coo_matrix((data, (row, col)), shape=(4, 4)) mtx.todense() Output:\n\u0026gt;\u0026gt;\u0026gt; mtx.todense() matrix([[3, 0, 1, 0], [0, 2, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1]]) no slicing…: mtx[2, 3] Output:\n\u0026gt;\u0026gt;\u0026gt; mtx[2, 3] Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; TypeError: \u0026#39;coo_matrix\u0026#39; object is not subscriptable An example sparse matrix arising from a finite difference discretistaion Many matrices in scientific computing contain mostly zeros, particularly those arising from the discretistaion of partial differential equations (PDEs). Here we will construct a sparse matrix using scipy.sparse that is derived from the finite difference discretistaion of the Poisson equation. In 1D, Poisson equation is\n\\[u_{xx} = f(x)\\text{ for }0 \\le x \\le 1\\]\nThe central FD approximation of $u_{xx}$ is:\n\\[u_{xx} \\approx \\frac{u(x + h) - 2u(x) + u(x-h)}{h^2}\\]\nWe will discretise $u_{xx} = 0$ at $N$ regular points along $x$ from 0 to 1, given by $x_1$, $x_2$:\n +----+----+----------+----+\u0026gt; x 0 x_1 x_2 ... x_N 1 Using this set of point and the discretised eqution, this gives a set of $N$ equations at each interior point on the domain:\n\\[\\frac{v_{i+1} - 2v_i + v_{i-1}}{h^2} = 0 \\text{ for } i = 1...N\\]\nwhere $v_i \\approx u(x_i)$.\nTo solve these equations we will need additional equations at $x=0$ and $x=1$, known as the boundary conditions. For this example we will use $u(x) = g(x)$ at $x=0$ and $x=1$ (also known as a non-homogenous Dirichlet bc), so that $v_0 = g(0)$, and $v_{N+1} = g(1)$, and the equation at $x_1$ becomes:\n\\[\\frac{v_{i+1} - 2v_i + g(0)}{h^2} = 0\\]\nand the equation at $x_N$ becomes:\n\\[\\frac{g(1) - 2v_i + v_{i-1}}{h^2} = 0\\]\nWe can therefore represent the final $N$ equations in matrix form like so:\n\\[ \\frac{1}{h^2} \\begin{bmatrix} -2 \u0026 1 \u0026 \u0026 \u0026 \\\\ 1 \u0026 -2 \u0026 1 \u0026 \u0026 \\\\ \u0026\\ddots \u0026 \\ddots \u0026 \\ddots \u0026\\\\ \u0026 \u0026 1 \u0026 -2 \u0026 1 \\\\ \u0026 \u0026 \u0026 1 \u0026 -2 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_{N-1}\\\\ v_{N} \\end{bmatrix} = \\begin{bmatrix} -g(0) \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ -g(1) \\end{bmatrix} \\]\nThe relevent sparse matrix here is $A$, given by\n\\[ A = \\begin{bmatrix} -2 \u0026 1 \u0026 \u0026 \u0026 \\\\ 1 \u0026 -2 \u0026 1 \u0026 \u0026 \\\\ \u0026\\ddots \u0026 \\ddots \u0026 \\ddots \u0026\\\\ \u0026 \u0026 1 \u0026 -2 \u0026 1 \\\\ \u0026 \u0026 \u0026 1 \u0026 -2 \\end{bmatrix} \\]\nAs you can see, the number of non-zero elements grows linearly with the size $N$, so a sparse matrix format is much prefered over a dense matrix holding all $N^2$ elements!\nAdditional Reading For more on the Finite Difference Method for solving PDEs:\nK. W. Morton and D. F. Mayers. Numerical Solution of Partial Differential Equations: An Introduction. Cambridge University Press, 2005.\nSoftware There are seven available sparse matrix types in scipy.sparse:\n csc_matrix: Compressed Sparse Column format csr_matrix: Compressed Sparse Row format bsr_matrix: Block Sparse Row format lil_matrix: List of Lists format dok_matrix: Dictionary of Keys format coo_matrix: COOrdinate format (aka IJV, triplet format) dia_matrix: DIAgonal format  As indicated by the excellent documentation, the dok_matrix or lil_matrix formats are preferable to construct matrices as they support basic slicing and indexing similar to a standard NumPy array.\nYou will notice that the FD matrix we have constructed for the Poisson problem is composed entirely of diagonal elements, as is often the case. If you were constructing a similar matrix in MATLAB, you would use the spdiags function, and scipy.sparse has its own equivalent. However, all the scipy.sparse formats also have special methods setdiag which provide a more object-orientated method of doing the same thing.\nScipy has a few different direct solvers for sparse matrics, given below:\nspsolve: This solves $Ax=b$ where $A$ is converted into CSC or CSR form\nspsolve_triangular: Solves $Ax=b$, where $A$ is assumed to be triangular.\nfactorized: This computes the $LU$ decomposition of the input matrix $A$, returning a Python function that can be called to solve $Ax = b$\nsplu: This computes the $LU$ decomposition of the input matrix $A$ using the popular SuperLU library. It returns a Python object of class SuperLU, that has a solve method you can use to solve $Ax = b$\nNote, scipy.sparse.linalg also has many iterative solvers, which we will investigate further in the next chapter.\nProblem: construction in scipy.sparse Your goal for this problem is to construct the FD matrix $A$ given above, using scipy.sparse, and:\n Visualise the matrix $A$ using the Matplotlib spy plot Solve the Poisson problem using $f(x) = 2 \\cos(x) / e^x$ and $g(x) = \\sin(x) / e^x$. Check your answer using the analytical solution $u_{a}(x) = \\sin(x) / e^x$. Vary the number of discretisation points $N$ and calculate $AA$ using both sparse and dense matrices. For each $N$ calculate the time to calculate the matix multiplicatiion using Python's time.perf_counter, and plot time verus $N$ for dense and sparse matrix multiplicatiion. Comment on how the time varies with $N$. Vary the number of discretisation points $N$ and solve the Poisson problem with varying $N$, and with using both the sparse and direct $LU$ solvers. For each $N$ record the time taken for both the dense and sparse solvers, and record the numerical error $||\\mathbf{v} - \\mathbf{v}_a||_2$. Generate plots of both error and time versus $N$, and comment on how they vary with $N$  "
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_1_linear_algebra/",
	"title": "Direct Solvers and Matrix Decompositions",
	"tags": [],
	"description": "",
	"content": "Chapter 1 Direct Solvers and Matrix Decompositions Warning Chapter under construction. Original course available here.\n "
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_1_linear_algebra/02-gaussian-elimination/",
	"title": "Gaussian Elimination",
	"tags": [],
	"description": "",
	"content": "Gaussian Elimination Consider the problem: Find $x, y,z$ such that\n\\[ \\begin{aligned} eq1: \u0026 2x \u0026 + \u0026 y \u0026 -\u0026 z \u0026 = \u0026 3 \\\\ eq2: \u0026 x \u0026 \u0026 \u0026+ \u00265z \u0026 = \u0026 6 \\\\ eq3: \u0026 -x \u0026+\u0026 3y\u0026 -\u0026 2z \u0026 = \u0026 3 \\end{aligned} \\]\nGaussian elimination -- step 1 reduce the above system of equations so that the unknown $x$ is removed from the last two equations:\n\\[ \\begin{aligned} eq1: \u0026 2x \u0026 +\u0026 y\u0026 -\u0026 z \u0026 = \u00263 \\\\ eq2 ~\\rightarrow eq1 - 2 \\times eq2: \u0026 \u0026 \u0026 y \u0026 - \u002611 z \u0026 =\u0026 -9 \\\\ eq3~ \\rightarrow ~ eq1 + 2 \\times eq3: \u0026 \u0026 \u0026 7y \u0026- \u00265z\u0026 = \u0026 9 \\end{aligned} \\]\nIn this case the 2 coefficient for x in the first row is the pivot, and we are using this pivot value to convert the other two x coefficients in the rows below to zeros.\nGaussian elimination -- step 2 remove the unknown $y$ from the last equation:\n\\[ \\begin{aligned} eq1: \u0026 2x + y - z \u0026 = 3 \\\\ eq2: \u0026 ~~~ y - 11 z \u0026 = -9 \\\\ eq3 ~ \\rightarrow ~ 7 \\times eq2 - eq3: \u0026 ~~~ -72 z = -72 \\end{aligned} \\]\nNow the pivot is the 1 coefficient for $y$ in eq2.\n\\[ \\begin{aligned} eq1: \u0026 2x + y - z \u0026 = 3 \\\\ eq2: \u0026 ~~~ y - 11 z \u0026 = -9 \\\\ eq3: \u0026 ~~~ -72 z \u0026 = -72 \\end{aligned} \\]\nThis system is said to be upper triangular. It is also known as row echelon form, and the leading coefficients ([2, 1, -72] in this case) are known as the pivots.\nGaussian elimination -- step 3 We can now use back substitution to obtain $x,y,z$.\nIn this case\n\\( z = 1,\\) \\( eq2: ~~ y - 11 = -9 ~~ \\implies ~~ y = 2,\\) \\( eq1: ~~ 2x +2 -1 = 3 , ~~ \\implies ~~ x = 1.\\)\nPivoting Consider the following system\n\\[ \\begin{aligned} eq1: \u0026 x \u0026 + \u0026 y \u0026 + \u0026 z \u0026 = \u0026 a \\\\ eq2: \u0026 2x \u0026 + \u0026 2y \u0026 + \u0026 5z \u0026 = \u0026 b \\\\ eq3: \u0026 4x \u0026 +\u0026 6y \u0026 + \u0026 8z \u0026 = \u0026 c \\end{aligned} \\]\nThis firstly reduces to\n\\[ \\begin{aligned} eq1: \u0026 x \u0026 + \u0026 y \u0026 + \u0026 z \u0026 = \u0026 a \\\\ eq2:\u0026 \u0026 \u0026 \u0026 \u0026 3z \u0026 = \u0026 b' \\\\ eq3: \u0026 \u0026 \u0026 2y \u0026 + \u0026 4z \u0026 = \u0026 c' \\end{aligned} \\]\nThe problem here is that we have zero for the pivot in eq2. This can easily be switched into upper triangular form by switching rows two and three.\nPartial pivoting: In general, we should be worried about both zero and very small pivot values, as in the latter case they will lead to division by a small value, which can cause large roundoff errors. So common practice is to select a row/pivot value such that the pivot value is as large as possible\nSingular matrices in Gaussian Elimination \\[ \\begin{aligned} eq1: \u0026 x \u0026 + \u0026 y \u0026 + \u0026 z \u0026 = \u0026 a \\\\ eq2: \u0026 2x \u0026 + \u0026 2y \u0026 + \u0026 5z \u0026 = \u0026 b \\\\ eq3: \u0026 4x \u0026 +\u0026 4y \u0026 + \u0026 8z \u0026 = \u0026 c \\end{aligned} \\]\nThis firstly reduces to\n\\[ \\begin{aligned} eq1: \u0026 x \u0026 + \u0026 y \u0026 + \u0026 z \u0026 = \u0026 a \\\\ eq2:\u0026 \u0026 \u0026 \u0026 \u0026 3z \u0026 = \u0026 b' \\\\ eq3:\u0026 \u0026 \u0026 \u0026 \u0026 4z \u0026 = \u0026 c' \\end{aligned} \\]\nWe cannot solve this by switching rows in this case, which means that the matrix is singular and has no inverse\nGaussian Elimination Rules  Operate on LHS and RHS (or RHSs) at the same time Replace row with a sum/combination of rows Work on one column at a time, choosing a pivot (leading non-zero entry in a chosen row), and eliminating all other non-zero values below that Switch rows to avoid zeros on the diagonal (pivoting) If (3) does not work, zeros on the diagonal (pivots) indicate a singular matrix  Computational cost: If the number of equations $n$ is large, then a number of operations for gaussian elimination is $\\mathcal{O}(n^3)$.\nPseudocode Wikipedia has a pseudocode implementation of the gaussian elimination algorithm which is helpful to understand how it works:\nh := 1 /* Initialization of the pivot row */ k := 1 /* Initialization of the pivot column */ while h ≤ m and k ≤ n /* Find the k-th pivot: */ i_max := argmax (i = h ... m, abs(A[i, k])) if A[i_max, k] = 0 /* No pivot in this column, pass to next column */ k := k+1 else swap rows(h, i_max) /* Do for all rows below pivot: */ for i = h + 1 ... m: f := A[i, k] / A[h, k] /* Fill with zeros the lower part of pivot column: */ A[i, k] := 0 /* Do for all remaining elements in current row: */ for j = k + 1 ... n: A[i, j] := A[i, j] - A[h, j] * f /* Increase pivot row and column */ h := h + 1 k := k + 1 Problems  Code a Python function that takes an 2D numpy array representing a matrix $A$, and a 1D array representing a RHS $b$, and returns the solution of the linear equation $Ax = b$. If you wish you can assume that the matrix has an inverse. Try it out on a few test matrices and check your answer using scipy.linalg.solve.  Condition Number Gaussian Elimination might still fail if $A$ is close to being singular, if a slight change to its values causes it to be singular. In this case simple round-off error in the floating point calculations can lead to zeros in the pivot positions.\nEven if the pivot value is not exactly zero, a pivot value close to zero can lead to large differences in the final result. In this case the matrix would be nearly singular, or ill-conditioned. Most linear algebra packages will include a method of calculating the condition number of a matrix, which evaluates how sensitive the solution is to the input values of the matrix or rhs vector. An identity matrix has a condition number of 1, while an exactly singular matrix has a condition number of infinity.\nProblems Suppose an experiment leads to the following system of equations:\n\\[ \\begin{aligned} 4.5 x_1 + 3.1 x_2 \u0026= 19.249, (1)\\\\ 1.6 x_1 + 1.1 x_2 \u0026= 6.843. \\end{aligned} \\]\n Solve system (1), and then solve system (2), below, in which the data on the right have been rounded to two decimal places. In each case, find the exact solution.  \\[ \\begin{aligned} 4.5 x_1 + 3.1 x_2 \u0026= 19.25, (2)\\\\ 1.6 x_1 + 1.1 x_2 \u0026= 6.84. \\end{aligned} \\]\n The entries in (2) differ from those in (1) by less than .05%. Find the percentage error when using the solution of (2) as an approximation for the solution of (2).\n Use numpy.linalg.cond to produce the condition number of the coefficient matrix in (1).\n  "
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/02-coo-matrix/",
	"title": "COOrdinate format",
	"tags": [],
	"description": "",
	"content": "As an example of a sparse matrix format, this section describes one of the sparse formats implemented in Scipy, the The COOrdinate format (COO). This is also known as the \u0026quot;ijv\u0026quot; or \u0026quot;triplet\u0026quot; format, and stores the non-zero elements in three arrays, row, col, and data. The data[i] value is the non-zero entry in row row[i] and column col[i] of the matrix. The advantages of this format are:\n fast format for contructing sparse matrices fast conversions to/from the CSR and CSC formats fast matrix-vector multiplication fast elementwise operations (e.g. multiply each element by 2 is just data * 2)  However, slicing using this format is difficult.\nHere are some examples of the COO matrix format using scipy.sparse.coo_matrix. Again, these have been taken from scipy-lectures, which is an excellent resource and contains examples of the other sparse matrix formats implemented in Scipy.\ncreate empty COO matrix: mtx = sparse.coo_matrix((3, 4), dtype=np.int8) mtx.todense() Output:\nmatrix([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], dtype=int8) create using (data, ij) tuple: row = np.array([0, 3, 1, 0]) col = np.array([0, 3, 1, 2]) data = np.array([4, 5, 7, 9]) mtx = sparse.coo_matrix((data, (row, col)), shape=(4, 4)) mtx mtx.todense() Output:\n\u0026gt;\u0026gt;\u0026gt; mtx \u0026lt;4x4 sparse matrix of type \u0026#39;\u0026lt;class \u0026#39;numpy.int64\u0026#39;\u0026gt;\u0026#39; with 4 stored elements in COOrdinate format\u0026gt; \u0026gt;\u0026gt;\u0026gt; mtx.todense() matrix([[4, 0, 9, 0], [0, 7, 0, 0], [0, 0, 0, 0], [0, 0, 0, 5]]) duplicates entries are summed together: row = np.array([0, 0, 1, 3, 1, 0, 0]) col = np.array([0, 2, 1, 3, 1, 0, 0]) data = np.array([1, 1, 1, 1, 1, 1, 1]) mtx = sparse.coo_matrix((data, (row, col)), shape=(4, 4)) mtx.todense() Output:\n\u0026gt;\u0026gt;\u0026gt; mtx.todense() matrix([[3, 0, 1, 0], [0, 2, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1]]) no slicing…: mtx[2, 3] Output:\n\u0026gt;\u0026gt;\u0026gt; mtx[2, 3] Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; TypeError: \u0026#39;coo_matrix\u0026#39; object is not subscriptable"
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_4_optimisation/02-line-search-methods/",
	"title": "Line Search Methods",
	"tags": [],
	"description": "",
	"content": "Gradient descent One of the simplest local optimisation algoriths is gradient descent. It is initialised at some point in parameter space $a_0$, and at each iteration the function $f(x)$ is reduced by following the direction of steepest descent $-\\nabla f(a)$\n\\[ a_{n+1} = a_n - \\gamma \\nabla f(a_n) \\]\nThis is an example of an imporant class of algorithms called the line search methods. These algorithms choose a search direction $p_k$ at each iteration $k$, and search along the 1D line from the initial point $a_k$ to a new point\n\\[ a_{k+1} = a_k + \\alpha p_k \\]\nwith a lower function value. The problem at each iteration becomes a one-dimensional optimisation problem along $p_k$ to find the optimal value of $\\alpha$. Each line search algorithm is thus defined on how it chooses both the search direction $p_k$ and the optimal $\\alpha$.\n  Illustration of Gradient Descent on a 2D test function. Taken from Wikimedia Commons   Plataus with low gradient An obvious downside to simple gradient descent can be seen for functions which have regions of zero or small gradients, or plataus. Here a gradient descent algorithm with a constant $\\gamma$ will proceed very slowly, if at all. This motivates another important line search algorithm, Newtons method.\nThe Newtons direction $p^N_k$ can be derived by considering the second-order Taylor expansion of the function $f(x)$\n\\[ f(a_k + p) \\approx f(a_k) + p^T \\nabla f(a_k) + \\frac{1}{2} p^T \\nabla^2 f(a_k) = m_k(p). \\]\nWe find the value of $p$ that minimises $m_k(p)$ by setting the derivative of $m_k$ to zero, leading to\n\\[ p_k^N = - (\\nabla^2 f(a_k))^{-1} \\nabla f(a_k) \\]\nUnlike the steepest descent, Newtons method has a natural step length $\\alpha \\approx 1$, which is suitable for a wide variety of problems and can quickly cross areas of low gradient. Natually, since the algorithm is based on a second-order approximation of the function $f$, it works better if this approximation is reasonably accurate.\nNewtons method can be used as long as the inverse of the second derivative of the function $(\\nabla^2 f(a_k))^{-1}$, exists (e.g. it will always exist for a positive definite $\\nabla^2 f$). However, even when this inverse does exist it is possible that the direction $p^N_k$ does not satisfy the descent condition $f(a_k + \\alpha p^N_k) \u0026lt; f(a_k)$ (or equivilently $\\nabla f(a_k)^T p^N \u0026lt; 0$), so many modifications to Newtons methods, falling under a class of methods called Quasi-Newton methods, have been proposed to satisfy this descent condition.\nQuasi-Newton methods do not require the (often onerous) calculation of the hession $\\nabla^2 f(x)$ like Newtons, instead they form an approximation to the hessian $B_k \\approx \\nabla^2 f(a_k)$ that is updated at each step using the information given by the gradient evaluations $\\nabla f(a_k)$. Two popular methods of performing this update are the symmetric-rank-one (SR1), and the Broyden, Fletcher, Goldfarb, and Shanno, (BFGS) formula. Once the approximation $B_k$ is formed then the search direction is calculated via\n\\[ p_k = -B_k^{-1} \\nabla f(a_k) \\]\nFor more details of other line search methods, and other concepts such as the Wolfe conditions for calculating the step length $\\alpha$, please see Chapter 3 of the Nocedal and Wright textbook, or in the other textbooks listed at the end of this lession. Finally, it should be noted that the conjugate gradient method can also be used for non-linear optimisation, where the search direction is given by\n\\[ p_k = -\\nabla f(a_k) + \\beta_k p_{k-1} \\]\nStep length In line search methods, choosing the step length $\\alpha_k$ is a non-trivial task. Ideally we would want to chose $\\alpha_k$ to minimise the function along the one-dimensional search direction $p_k$. That is, we wish to minimise\n\\[ \\phi(\\alpha_k) = f(a_k + \\alpha_k p_k),\\text{ }\\alpha_k  0. \\]\nIn general this is too expensive to do this minimisation exactly, so approximate methods are used so that multiple trial $\\alpha_k$ values are trialed, stopping when a candidate is found that satisfies a set of conditions. There are two main conditions used, the Wolfe conditions and the Goldstein conditions.\nThe two Wolfe conditions are the sufficient decrease condition, which ensures that the reduction in the function value is proportional to the step length $\\alpha_k$ and the gradient in the direction of the step\n\\[ f(a_k + \\alpha_k p_k) \\le f(a_k) + c_1 \\alpha_k \\nabla f(a_k)^T p_k. \\]\nThe second Wolfe condition is the curvature condition, which prevents unacceptibly short steps by ensuring that the slope of $\\phi$ is greater than some constant $c_2$ times the initial slope $\\phi'(0)$\n\\[ \\nabla f(a_k + \\alpha_k p_k)^T p_k \\ge c_2 \\nabla f(a_k)^T p_k, \\]\nwhere $c_2 \\in (c_1, 1)$. Typical values are $c_1 = 10^{-4}$ and $c_2 = 0.9$. The strong Wolfe conditions restrict the gradient $\\phi'$ to be small, so as to exclude points that are too far from stationary points of $\\phi$\n\\[ f(a_k + \\alpha_k p_k) \\le f(a_k) + c_1 \\alpha_k \\nabla f(a_k)^T p_k. \\]\n\\[ |\\nabla f(a_k + \\alpha_k p_k)^T p_k| \\ge c_2 |\\nabla f(a_k)^T p_k|, \\]\nThe Goldstein conditions are similar in spirit to the Wolfe condtitions, and are formed from the two inequalities\n\\[ f(a_k) + (1 - c) \\alpha_k \\nabla f(a_k)^T p_k \\le f(a_k + \\alpha_k p_k) \\le f(a_k) + c \\alpha_k \\nabla f(a_k)^T p_k. \\]\nwith $0 \u0026lt; c \u0026lt; 1/2$. The first inequality prevents small step sizes while the second is the same sufficient decrease condition as in the Wolfe conditions. The Goldstein conditions are often used in Newton-type methods but for quasi-Newton methods the Wolfe conditions are prefered. The diagrams from the text by Nocedal and Wright illustrate the two conditions\n\nAlgorithms for choosing candidate step size values $\\alpha_k$ can be complicated, so we will only mention here one of the simplest, which is the backtracking method. This approach implicitly satisfies the condition on too small $\\alpha_k$, and only repeatedly test for the common sufficient decrease condition that appears in both the Wolfe and Goldstein condtitions.\nChoose $\\bar{\\alpha} \u0026gt; 0$, $\\rho \\in (0, 1)$, $c \\in (0, 1)$ $\\alpha_k := \\bar{\\alpha}$ repeat until $f(a_k + \\alpha_k p_k) \\le f(a_k) + c \\alpha_k \\nabla f(a_k)^T p_k$ \u0026nbsp;\u0026nbsp; $\\alpha_k := \\rho \\alpha_k$ end repeat\nSoftware  Scipy has a wide variety of (mostly) line search and trust region algorithms in scipy.optimize  Problems  Program the steepest descent and Newton algorithms using the backtracking line search, Algorithm 3.1. Use them to minimize the Rosenbrock function. Set the initial step length $\\alpha_0 = 1$ and print the step length used by each method at each iteration. First try the initial point $x_0 = (1.2, 1.2)^T$ and then the more difficult starting point $x_0 = (−1.2, 1)^T$. Plot the function surface using matplotlib and overlay the line search segments so you can visualise the progress of your algorithm. Repeat (1) and (2) above using the line seach implemented in Scipy scipy.optimize.line_search, which uses the strong Wolfe conditions.  "
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/",
	"title": "Sparse Matrices and Iterative Solvers",
	"tags": [],
	"description": "",
	"content": "Chapter 1 Sparse Matrices and Iterative Solvers Warning Chapter under construction. Original course available here.\n "
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_1_linear_algebra/03-matrix-decompositions/",
	"title": "Matrix decompositions",
	"tags": [],
	"description": "",
	"content": "Matrix factorisations play a key role in the solution of problems of the type $A x = b$. Often (e.g. ODE solvers), you have a fixed matrix $A$ that must be solved with many different $b$ vectors. A matrix factorisation is effectivly a pre-processing step that allows you to partition $A$ into multiple factors (e.g. $A = LU$ in the case of $LU$ decomposition), so that the actual solve is as quick as possible. Different decompositions have other uses besides solving $A x = b$, for example:\n the $LU$, $QR$ and Cholesky decomposition can be used to quickly find the determinant of a large matrix, since $\\det(AB) = \\det(A) \\det(B)$. The Cholesky decomposition can be used to sample from a multivariate normal distribution, and is a very efficient technique to solve $A x = b$ for the specific case of a positive definite matrix. The $QR$ decomposition can be used to solve a minimum least squares problem, to find the eigenvalues and eigenvectors of a matrix, and to calulcate the Singular Value Decomposition (SVD), which is itself another very useful decomposition!  "
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/03-finite-difference/",
	"title": "Finite Difference Matrix",
	"tags": [],
	"description": "",
	"content": "Many matrices in scientific computing contain mostly zeros, particularly those arising from the discretistaion of partial differential equations (PDEs). Here we will construct a sparse matrix using scipy.sparse that is derived from the finite difference discretistaion of the Poisson equation. In 1D, Poisson equation is\n\\[u_{xx} = f(x)\\text{ for }0 \\le x \\le 1\\]\nThe central FD approximation of $u_{xx}$ is:\n\\[u_{xx} \\approx \\frac{u(x + h) - 2u(x) + u(x-h)}{h^2}\\]\nWe will discretise $u_{xx} = 0$ at $N$ regular points along $x$ from 0 to 1, given by $x_1$, $x_2$:\n +----+----+----------+----+\u0026gt; x 0 x_1 x_2 ... x_N 1 Using this set of point and the discretised eqution, this gives a set of $N$ equations at each interior point on the domain:\n\\[\\frac{v_{i+1} - 2v_i + v_{i-1}}{h^2} = 0 \\text{ for } i = 1...N\\]\nwhere $v_i \\approx u(x_i)$.\nTo solve these equations we will need additional equations at $x=0$ and $x=1$, known as the boundary conditions. For this example we will use $u(x) = g(x)$ at $x=0$ and $x=1$ (also known as a non-homogenous Dirichlet bc), so that $v_0 = g(0)$, and $v_{N+1} = g(1)$, and the equation at $x_1$ becomes:\n\\[\\frac{v_{i+1} - 2v_i + g(0)}{h^2} = 0\\]\nand the equation at $x_N$ becomes:\n\\[\\frac{g(1) - 2v_i + v_{i-1}}{h^2} = 0\\]\nWe can therefore represent the final $N$ equations in matrix form like so:\n\\[ \\frac{1}{h^2} \\begin{bmatrix} -2 \u0026 1 \u0026 \u0026 \u0026 \\\\ 1 \u0026 -2 \u0026 1 \u0026 \u0026 \\\\ \u0026\\ddots \u0026 \\ddots \u0026 \\ddots \u0026\\\\ \u0026 \u0026 1 \u0026 -2 \u0026 1 \\\\ \u0026 \u0026 \u0026 1 \u0026 -2 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_{N-1}\\\\ v_{N} \\end{bmatrix} = \\begin{bmatrix} -g(0) \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ -g(1) \\end{bmatrix} \\]\nThe relevent sparse matrix here is $A$, given by\n\\[ A = \\begin{bmatrix} -2 \u0026 1 \u0026 \u0026 \u0026 \\\\ 1 \u0026 -2 \u0026 1 \u0026 \u0026 \\\\ \u0026\\ddots \u0026 \\ddots \u0026 \\ddots \u0026\\\\ \u0026 \u0026 1 \u0026 -2 \u0026 1 \\\\ \u0026 \u0026 \u0026 1 \u0026 -2 \\end{bmatrix} \\]\nAs you can see, the number of non-zero elements grows linearly with the size $N$, so a sparse matrix format is much prefered over a dense matrix holding all $N^2$ elements!\nAdditional Reading For more on the Finite Difference Method for solving PDEs:\nK. W. Morton and D. F. Mayers. Numerical Solution of Partial Differential Equations: An Introduction. Cambridge University Press, 2005.\n"
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_4_optimisation/03-trust-region-methods/",
	"title": "Trust Region Methods",
	"tags": [],
	"description": "",
	"content": "Saddle points Saddle point pose a particular challenge in non-linear optimisation, particularly in higher dimensions. The plots below show two examples of saddle points in two dimensions. Like local minima and maxima, these are stationary points where the gradient of the function is zero $\\nabla f = 0$, but where the value of the function rises along certain directions and reduces along others (left plot). An alternative type of saddle point arises when the hessian is singular, and are characterised by a plateau around the stationary point, like the monkey saddle depicted in the plot to the right.\n\nGradient descent perform poorly, with very slow convergence near saddle points, and Newtons methods tend to be attracted and trapped in saddle points, due to the presence of negative eigenvalues in the hessian matrix. This can be avoided by approximating the hessian with a positive definite matrix $B_k \\approx \\nabla^2 f$, like some quasi-Newton methods (i.e. BFGS). Alternativly, the class of trust region methods restate the optimisation problem as an sequence of optimisations of a second order approximation to $f$ in a local trust-region surrounding the current point $a_k$. The exact solution to each of these subproblems can be shown to be $(\\nabla^2 f(a_k) + \\lambda I)^{-1} \\nabla f(a_k)$, where $\\lambda$ is chosen to be large enought to make $(\\nabla^2 f(a_k) + \\lambda I)$ positive definite. Therefore, by design the trust-region methods aim avoid this problem of negative eigenvalues.\nTrust region methods Like many line search methods, trust region methods also use the second order Taylor expansion of $f$ around $a_k$\n\\[ f(a_k + p) \\approx f(a_k) + g_k^T p + \\frac{1}{2} p^T B_k p = m_k(p) \\]\nwhere $g_k = \\nabla f(a_k)$, $B_k$ is an approximation to the hessian matrix $B_k \\approx \\nabla^2 f(a_k)$ or the hessian itself $B_k = \\nabla^2 f(a_k)$. Trust region methods aim to find the $p$ that minimises $m_k$ in a local trust region $||p|| \u0026lt; \\Delta_k$ around the current point $a_k$, where $\\Delta_k$ is the trust region radius.\nSolving the minimisation given above is normally done approximately, with different trust region methods varying how the approximation is achieved. Choosing the trust-region radius is fundamental to this class of methods, and is done by comparing the actual to the predicted reduction in the function value\n\\[ \\rho_k = \\frac{f(a_k) - f(a_k + p_k)}{m_k(0) - m_k(p_k)}. \\]\nSince $m_k(0) - m_k(p_k)$ is always positive, if $\\rho_k$ is negative then the the actual function value is increasing, the step is rejected and the trust region radius $\\Delta_k$ is decreased in order to improve the approximate model $m_k$. If $\\rho_k$ is positive but much smaller than one then we do not alter $\\Delta_k$. If $\\rho_k$ is close to or greater than 1 we can be confident in our model and thus increase $\\Delta_k$. The general algorithm for a trust region method (reproduced from the text by Nocedal and Wright cited below) is:\nGiven $a_0$, $\\hat{\\Delta} \u0026gt; 0$, $\\Delta_0 \\in (0, \\hat{\\Delta})$, and $\\nu \\in [0, \\frac{1}{4})$: for $k = 0, 1, 2, ...$ \u0026nbsp;\u0026nbsp; Obtain $p_k$ by (approximatly) minimising $m_k(p)$ where $||p|| \u0026lt; \\Delta_k$ \u0026nbsp;\u0026nbsp; $\\rho_k := \\frac{f(a_k) - f(a_k + p_k)}{m_k(0) - m_k(p_k)}$ \u0026nbsp;\u0026nbsp; if $\\rho_k \u0026lt; \\frac{1}{4}$ \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; $\\Delta_{k+1} := \\frac{1}{4} \\Delta_k$ \u0026nbsp;\u0026nbsp; else \u0026nbsp;\u0026nbsp; \u0026nbsp;\u0026nbsp;if $\\rho_k \u0026gt; \\frac{3}{4}$ and $||p_k|| = \\Delta_k$ \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026nbsp;\u0026nbsp; $\\Delta_{k+1} := \\min(2 \\Delta_k, \\hat{\\Delta})$ \u0026nbsp;\u0026nbsp; \u0026nbsp;\u0026nbsp;else \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026nbsp;\u0026nbsp; $\\Delta_{k+1} := \\Delta_k$ \u0026nbsp;\u0026nbsp; if $\\rho_k \u0026gt; \\nu$ \u0026nbsp;\u0026nbsp; \u0026nbsp;\u0026nbsp; $a_{k+1} := a_k + p_k$\n\u0026nbsp;\u0026nbsp;else \u0026nbsp;\u0026nbsp; \u0026nbsp;\u0026nbsp; $a_{k+1} := a_k$\nend for\nWe will describe two algorithms for minimising $m_k(p)$, the Cauchy point and the dogleg methods. The Cauchy point first solves a linear version of $m_k$ defined as\n\\[ p^s_k = \\min_{p \\in \\mathcal{R}^n} f(a_k) + g_k^T p \\text{ for }||p|| \\le \\Delta_k \\]\nSubsequently, $p^s_k$ is used to find the scalar $\\tau_k \u0026gt; 0$ such that\n\\[ \\tau_k = \\min_{\\tau \\ge 0} m_k(\\tau p_k^s) \\text{ for }||\\tau p_k^s|| \\le \\Delta_k \\]\nFinally, the Cauchy point is given as $p_k^C = \\tau_k p_k^s$.\nThe solution to this problem can be shown to be\n\\[ p_k^C = -\\tau_k \\frac{\\Delta_k}{|| g_k ||} g_k, \\]\nwhere\n\\[ \\tau_k = \\begin{cases} 1 \u0026 \\text{if }g_k^T B_k g_k \\le 0 \\\\ \\min (||g_k||^3 / (\\Delta_k g_k^T B_k g_k), 1) \u0026 \\text{otherwise}. \\end{cases} \\]\nThe second method we describe is the dogleg method, which is applicable when $B_k$ is a positive definite matrix. If the original hessian is positive definite then this method is directly applicable, or one of the quasi-Newton positive definite approximation to the hessian could also be used. The dogleg method is derived by considering the path of the $p$ that minimises $m_k(p)$ with increasing $\\Delta_k$, which forms a curved path in parameter space. The method approximates this path with two straight line segments. The first segment follows the steepest descent direction and is given by\n\\[ p_k^U = -\\frac{g_k^T g_k}{g_k^T B_k g_k} g_k \\]\nThe second step is along the path between $p_k^U$ and $p^B_k = -B_k^{-1} g_k$. In the case where $p_k^B$ is inside the trust region $||p_k^B|| \\le \\Delta_k$ then $p_k^B$ can be used without modification. Otherwise the point of intersection with the trust-region radius must be calculated, which can be done by solving the following quadratic equation\n\\[ ||p_k^U + (\\tau - 1)(p_k^B - p_k^U)||^2 = \\Delta_k^2 \\]\nwith the second segment being defined by\n\\[ \\tilde{p}_k(\\tau) = \\begin{cases} \\tau p_k^U, \u0026 0 \\le \\tau 1, \\\\ p_k^U + (\\tau - 1)(p_k^B - p_k^U), \u0026 1 \\le \\tau \\le 2. \\end{cases} \\]\nProblems  Let $f(x) = 10 \\left( x_2 − x^2_1 \\right)^2 + (1 − x_1)^2$. At $x = (0,−1)$ draw the contour lines of the quadratic model   \\[ m_k(p) = f(a_k) + g_k^T p + \\frac{1}{2} p^T B_k p, \\]\nassuming that $B_k$ is the Hessian of $f$. Draw the family of solutions of $\\min_{p \\in \\mathcal{R}^n}m_k(p)$ so that $||p|| \\le \\Delta_k$ as the trust region radius varies from $\\Delta_k = 0$ to $\\Delta_k = 2$. Repeat this at $x = (0, 0.5)$.\nWrite a program that implements the dogleg method. Choose $B_k$ to be the exact Hessian. Apply it to solve Rosenbrock’s function. Experiment with the update rule for the trust region by changing the constants in the trust region algorithm given above, or by designing your own rules.  "
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_3_ode/",
	"title": "Solving ODEs",
	"tags": [],
	"description": "",
	"content": "Chapter 1 Solving Ordinary Differential Equations (ODEs) Warning Chapter under construction.\n "
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_1_linear_algebra/04-lu-decomposition/",
	"title": "LU decomposition",
	"tags": [],
	"description": "",
	"content": "$LU$ decomposition The $LU$ decomposition is closely related to gaussian elimination. It takes the original equation to be solved $A x = b$ and splits it up into two separate equations involving a unit lower triangular matrix $L$, and the row echelon matrix $U$:\n\\[ \\begin{aligned} L y \u0026= b \\\\ U x \u0026= y \\end{aligned} \\]\nwhere $A = LU$. The $L$ matrix is a unit lower triangular matrix and thus has ones on the diagonal, whereas $U$ is in row echelon form with pivot values in the leading coefficients of each row.\n\\[ A = \\left( \\begin{matrix} 1 \u0026 0 \u0026 0 \u0026 0 \\\\ \\ast \u0026 1 \u0026 0 \u0026 0 \\\\ \\ast \u0026 \\ast \u0026 1 \u0026 0 \\\\ \\ast \u0026 \\ast \u0026 \\ast \u0026 1 \\end{matrix}\\right) \\left(\\begin{matrix} p_1 \u0026 \\ast \u0026 \\ast \u0026 \\ast \\\\ 0 \u0026 p_2 \u0026 \\ast \u0026 \\ast \\\\ 0 \u0026 0 \u0026 p_3 \u0026 \\ast \\\\ 0 \u0026 0 \u0026 0 \u0026 p_4 \\end{matrix}\\right) \\]\nThus, we have converted our original problem of solving $A x = b$ into two separate solves, first solving the equation $L y = b$, and then using the result $y$ to solve $U x = y$.\n\\[ \\begin{aligned} \\left(\\begin{matrix} 1 \u0026 0 \u0026 0 \u0026 0 \\\\ \\ast \u0026 1 \u0026 0 \u0026 0 \\\\ \\ast \u0026 \\ast \u0026 1 \u0026 0 \\\\ \\ast \u0026 \\ast \u0026 \\ast \u0026 1 \\end{matrix}\\right) \\left(\\begin{matrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\end{matrix}\\right) \u0026= \\left(\\begin{matrix} b_1 \\\\ b_2 \\\\ b_3 \\\\ b_4 \\end{matrix}\\right) \\\\ \\left(\\begin{matrix} p_1 \u0026 \\ast \u0026 \\ast \u0026 \\ast \\\\ 0 \u0026 p_2 \u0026 \\ast \u0026 \\ast \\\\ 0 \u0026 0 \u0026 p_3 \u0026 \\ast \\\\ 0 \u0026 0 \u0026 0 \u0026 p_4 \\end{matrix}\\right) \\left(\\begin{matrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{matrix}\\right) \u0026= \\left(\\begin{matrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\end{matrix}\\right) \\end{aligned} \\]\nHowever, each of those solves is very cheap to compute, in this case for the 4x4 matrix shown above the solution of $L y = b$ only needs 6 multiplication and 6 additions, whereas $U x = y$ requires 4 divisions, 6 multiplications and 6 additions, leading to a total of 28 arithmetic operations, much fewer in comparison with the 62 operations required to solve the original equation $A x = b$. In general, $LU$ decomposition for an $n \\times n$ matrix takes about $2 n^3 / 3$ flops, or floating point operations, to compute.\n$LU$ factorisation without pivoting A relativly simple $LU$ algorithm can be described if we assume that no pivoting is required during a gaussian elimination. In this case, the gaussian elimination process is a sequence of $p$ linear operations $E_1, E_2, ..., E_p$, with each operation $E_i$ being a row replacement that adds a multiple of one row to another below it (i.e. $E_i$ is lower triangular). The final matrix after applying the sequence of row reductions is $U$ in row echelon form, that is:\n\\[ E_p \\cdots E_2 E_1 A = U \\]\nSince we have $A = LU$, we can show that the sequence of operations $E_1, E_2, ..., E_p$ is also the sequence that reduces the matrix $L$ to an identity matrix:\n\\[ A = (E_p \\cdots E_2 E_1)^{-1} U = LU, \\]\ntherefore,\n\\[ L = (E_p \\cdots E_2 E_1)^{-1}, \\]\nand,\n\\[ (E_p \\cdots E_2 E_1) L = (E_p \\cdots E_2 E_1) (E_p \\cdots E_2 E_1)^{-1} = I \\]\nThis implies how we can build up the matrix $L$. We choose values for $L$ such that the series of row operations $E_1, E_2, ..., E_p$ convert the matrix $L$ to the identity matrix. Since each $E_i$ is lower triangular, we know that both $(E_p \\cdots E_2 E_1)$ and $(E_p \\cdots E_2 E_1)^{-1}$ are also lower triangular.\nFor example, consider the following matrix\n\\[ A = \\left(\\begin{matrix} 3 \u0026 2 \u0026 1 \u0026 -3 \\\\ -6 \u0026 -2 \u0026 1 \u0026 5 \\\\ 3 \u0026 -4 \u0026 -7 \u0026 2 \\\\ -9 \u0026 -6 \u0026 -1 \u0026 15 \\end{matrix}\\right) \\]\nAfter three row reductions, $R_2 \\mathrel{{+}{=}} 2 R_1$, $R_3 \\mathrel{{+}{=}} -1 R_1$, and $R_3 \\mathrel{{+}{=}} 3 R_1$, we have the following result:\n\\[ E_1 E_2 E_3 A = \\left(\\begin{matrix} 3 \u0026 2 \u0026 1 \u0026 -3 \\\\ 0 \u0026 2 \u0026 * \u0026 * \\\\ 0 \u0026 -6 \u0026 * \u0026 * \\\\ 0 \u0026 0 \u0026 * \u0026 * \\end{matrix}\\right) \\]\nTo build the 1st column of $L$, we simply divide the 1st column of $A$ by the pivot value 3, giving\n\\[ L = \\left(\\begin{matrix} 1 \u0026 0 \u0026 0 \u0026 0 \\\\ -2 \u0026 1 \u0026 0 \u0026 0 \\\\ 1 \u0026 * \u0026 1 \u0026 0 \\\\ -3 \u0026 * \u0026 * \u0026 1 \\end{matrix}\\right) \\]\nFor the next column we do the same, using the new pivot value $A_{2,2} = 2$ in row 2 to reduce $A_{3,2}$ and $A_{4,2}$ to zero, and then dividing the column vector under the pivot $(-6, 0)^T$ by the pivot value 2 to obtain the next column of $L$.\nRepeating this process for all the columns in $A$, we obtain the final factorisation. You can verify for yourself that repeating the same row operations we did to form $U$ to the matrix $L$ reduces it to the identity matrix.\n\\[ L = \\left(\\begin{matrix} 1 \u0026 0 \u0026 0 \u0026 0 \\\\ -2 \u0026 1 \u0026 0 \u0026 0 \\\\ 1 \u0026 -3 \u0026 1 \u0026 0 \\\\ -3 \u0026 0 \u0026 2 \u0026 1 \\end{matrix}\\right) \\]\n\\[ E_1 E_2 ... E_p A = U = \\left(\\begin{matrix} 3 \u0026 2 \u0026 1 \u0026 -3 \\\\ 0 \u0026 2 \u0026 3 \u0026 -1 \\\\ 0 \u0026 0 \u0026 1 \u0026 2 \\\\ 0 \u0026 0 \u0026 0 \u0026 2 \\end{matrix}\\right) \\]\nPivoting Of course, for any practial $LU$ factorisation we need to consider pivoting. Any matrix $A$ can be factorised into $PLU$, where $P$ is a permutation matrix, and $L$ and $U$ are defined as before. During the gaussian elimination steps we store an array of row indices $p_i$ indicating that row $i$ is interchanged with row $p_i$, and the resultant array of $p_i$ can be used to build the permutation matrix $P$ (It would be wasteful to store the entire martix $P$ so the array $p_i$ is stored instead).\nThus, the LU algorithm proceeds as follows:\n Begin with the left-most column $i=0$, find an appropriate pivot (e.g. maximum entry in the colum) and designate this row as the pivot row. Interchange this row with row $i$, and store the pivot row index as $p_i$. Use row replacements to create zeros below the pivot. Create the corresponding column for $L$ by dividing by the pivot value. Continue along to the next column $i$, again choosing a pivot row $p_i$, interchanging it with row $i$ and creating zeros below the pivot, creating the new column in $L$, and making sure to record which pivot row has been chosen for each column. Repeat this step for all the columns of the matrix. Once the last column has been done, $U$ should be in row echlon form and $L$ should be a unit lower triangular matrix. The array $p_i$ implicitly defines the permutation matrix $P$  In practice, most library implementation store $L$ and $U$ in the same matrix since they are lower and upper triangular respectivly.\nOther Reading  Linear algebra and its applications by David C. Lay. Chaper 2.5\n Golub, G. H. \u0026amp; Van Loan, C. F. Matrix Computations, 3rd Ed. (Johns Hopkins University Press, 1996). Chapter 3.2 https://en.wikipedia.org/wiki/LU_decomposition  Software   scipy.linalg.lu_factor.  scipy.linalg.lu_solve.  scipy.linalg.lu.  Problems  Take your gaussian elimination code that you wrote in the previous lesson and use it to write an LU decomposition function that takes in a martix $A$, and returns $L$, $U$ and the array $p_i$. You can check your answer using scipy.linalg.lu_factor, or by simply verifying that $PLU=A$ Write a unit test or set of unit tests using the Python unittest framework that you can run to satisfy that your function is robust for a wide varietry of inputs $A$.  "
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_4_optimisation/04-derivatite-free-methods/",
	"title": "Derivative-free methods",
	"tags": [],
	"description": "",
	"content": "The line search and trust region methods introduced in the previous lesson all required that the user be able to calculate the gradient of the function $\\nabla f$. However, in many cases the gradient is either not available or too error-prone to be of use. For example, the function $f$ might be only available as a compiled executable or the result of a physical experiment. The model might be stochastic, or the gradient evaluation might be noisy due to numerical innacuracies, or of sufficiently complexity that the gradient is either unknown or too expensive to compute.\nHere we describe two of the most common methods for derivative-free optimisation, using a finite difference approximation to approximate the derivative, and the Nelder-Mead algorithm, which is a Simplex search method. However, there are a large number of derivative-free methods, ranging from the classical\nDirect Search methods like Pattern search, Simplex search, Rosenbrock' or Powell's methods. Then there are emulator or model -based methods that build up a model of the function $f$ and minimise that using a gradient-based method, a powerful example of this class of methods is Bayesian Optimisation. Many global optimsiation algorithms are derivative-free, including randomised algorithms such as Simulated Annealing, and evolution-based strategies such as the popular Covariance matrix adaptation evolution strategy (CMA-ES), or swarm algorithms inspired from bees/ants like Particle Swarm Optimisation.\n"
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_4_optimisation/05-finite-difference-method/",
	"title": "Finite difference method",
	"tags": [],
	"description": "",
	"content": "The simplest way of converting a gradient-based optimisation algorithm to a derivative free one is to approximate the gradient of the function using finite differences.\nThe Finite Difference (FD) method is based on taking a Taylor series expansion of either $f(x+h)$ and $f(x-h)$ (and others) for a small parameter $f$ about $x$. Consider a smooth function $f(x)$ then its Taylor expansion is\n\\[f(x+h) = f(x) + h f'(x) + \\frac{h^2}{2} f''(x) + \\frac{h^3}{6} f'''(x) + \\frac{h^4}{24} f'''''(x) + \\ldots \\]\n\\[f(x-h) = f(x) - h f'(x) + \\frac{h^2}{2} f''(x) - \\frac{h^3}{6} f'''(x) + \\frac{h^4}{24} f'''''(x) - \\ldots \\]\nFrom this, we can compute three different \\emph{schemes} (approximations) to $u'(x)$:\nForward difference: \\(u'(x) = \\frac{u(x+h)-u(x)}{h} + O(h)\\)\nBackward difference: \\(u'(x) = \\frac{u(x)-u(x-h)}{h} + O(h)\\)\nCentered difference: \\(u'(x) = \\frac{u(x+h)-u(x-h)}{2h} + O(h^2)\\)\nFinite difference approximations are easily computed, but suffer from innacuracies which can cause optimisation algorithms to fail or perform poorely. As well as the error in the FD approximation itself (e.g. $O(h^2)$ for centered difference), the function evaluation itself might have some numerical or stochastic \u0026quot;noise\u0026quot;. If this noise dominates over the (small) step size $h$, then it is entirely probable that the calculated steepest descent $-\\nabla f(x)$ will not be a direction of descent for $f$.\nSoftware It is very common that optimisation libraries provide a finite difference approximation to the Jacobian $\\nabla f$ if it is not supplied, as is done for the gradient-based methods in scipy.optimize.\nMore dedicated libraries can give superior approximations to the gradient, like the numdifftools package. This library provides higher order FD approximations and Richardson extrapolation to evaluate the limit of $h \\rightarrow 0$, and can calculate Jacobians and Hessians of user-supplied functions.\nProblems  Implement a simple 2D quadratic function $f(x, y) = x^2 + y^2$, the 2D Rosenbrock function, and the 2D Rastrigin function, and visualise these functions over the domain $-5 \u0026lt; x \u0026lt; 5$ and $-5 \u0026lt; y \u0026lt; 5$\n Use scipy.optimize to minimise each of these functions, starting at $x = y = 2.5$, using the following methods:\n Nelder-Mead Simplex Conjugate Gradient BFGS Quasi-Newton Newton-CG SHG Global Optimisation   In each case perform the optimisation with and without a user-supplied jacobian and evaluate the effect on the number of evaluations of the function $f$ required to converge to the opimium.\nTry using numdifftools to calculate the jacobian. See if you can improve on the number of function evaluations required to converge to an minimum.  "
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/04-scipy-sparse/",
	"title": "Scipy.sparse",
	"tags": [],
	"description": "",
	"content": "There are seven available sparse matrix types in scipy.sparse:\n csc_matrix: Compressed Sparse Column format csr_matrix: Compressed Sparse Row format bsr_matrix: Block Sparse Row format lil_matrix: List of Lists format dok_matrix: Dictionary of Keys format coo_matrix: COOrdinate format (aka IJV, triplet format) dia_matrix: DIAgonal format  As indicated by the excellent documentation, the dok_matrix or lil_matrix formats are preferable to construct matrices as they support basic slicing and indexing similar to a standard NumPy array.\nYou will notice that the FD matrix we have constructed for the Poisson problem is composed entirely of diagonal elements, as is often the case. If you were constructing a similar matrix in MATLAB, you would use the spdiags function, and scipy.sparse has its own equivalent. However, all the scipy.sparse formats also have special methods setdiag which provide a more object-orientated method of doing the same thing.\nScipy has a few different direct solvers for sparse matrics, given below:\nspsolve: This solves $Ax=b$ where $A$ is converted into CSC or CSR form\nspsolve_triangular: Solves $Ax=b$, where $A$ is assumed to be triangular.\nfactorized: This computes the $LU$ decomposition of the input matrix $A$, returning a Python function that can be called to solve $Ax = b$\nsplu: This computes the $LU$ decomposition of the input matrix $A$ using the popular SuperLU library. It returns a Python object of class SuperLU, that has a solve method you can use to solve $Ax = b$\nNote, scipy.sparse.linalg also has many iterative solvers, which we will investigate further in the next chapter.\nProblem: construction in scipy.sparse Your goal for this problem is to construct the FD matrix $A$ given in the Finite difference section, using scipy.sparse, and:\n Visualise the matrix $A$ using the Matplotlib spy plot Solve the Poisson problem using $f(x) = 2 \\cos(x) / e^x$ and $g(x) = \\sin(x) / e^x$. Check your answer using the analytical solution $u_{a}(x) = \\sin(x) / e^x$. Vary the number of discretisation points $N$ and calculate $AA$ using both sparse and dense matrices. For each $N$ calculate the time to calculate the matix multiplicatiion using Python's time.perf_counter, and plot time verus $N$ for dense and sparse matrix multiplicatiion. Comment on how the time varies with $N$. Vary the number of discretisation points $N$ and solve the Poisson problem with varying $N$, and with using both the sparse and direct $LU$ solvers. For each $N$ record the time taken for both the dense and sparse solvers, and record the numerical error $||\\mathbf{v} - \\mathbf{v}_a||_2$. Generate plots of both error and time versus $N$, and comment on how they vary with $N$  "
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_4_optimisation/",
	"title": "Non-linear Optimisation",
	"tags": [],
	"description": "",
	"content": "Chapter 1 Non-linear Optimisation Warning Chapter under construction.\n "
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_1_linear_algebra/05-ldl-decomposition/",
	"title": "LDL decomposition",
	"tags": [],
	"description": "",
	"content": "$LDL$ decomposition It is often very benificial when solving linear systems to consider and take advantage of any special structure that the matrix $A$ might possesses. The $LDL$ decomposition is a varient on LU decomposition which is only applicable to a symmetric matrix $A$ (i.e. $A = A^T$). The advantage of using this decomposition is that it takes advantage of the redundent entries in the matrix to reduce the amount of computation to $n^3/3$, which is about a half that required for the $LU$ decomposition.\nOther reading  Golub, G. H. \u0026amp; Van Loan, C. F. Matrix Computations, 3rd Ed. (Johns Hopkins University Press, 1996). Chapter 4.1 https://en.wikipedia.org/wiki/Cholesky_decomposition#LDL_decomposition_2  Software scipy.linalg.ldl\n"
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/05-iterative-methods/",
	"title": "Iterative methods",
	"tags": [],
	"description": "",
	"content": "Previously we have discussed direct linear algebra solvers based on decompositions of the original matrix $A$. The amount of computational effort required to achieve these decomposisions is $\\mathcal{O}(n^3)$, where $n$ is the number of rows of a square matrix. They are therefore unsuitable for the large, sparse systems of equations that are typically encountered in scientific applications. An alternate class of linear algebra solvers are the iterative methods, which produce a series of approximate solutions $x_k$ to the $A x = b$ problem. The performance of each algorithm is then based on how quickly, or how many iterations $k$ are required, for the solution $x_k$ to converge to within a set tolerance of the true solution $x$.\n"
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_5_bayesian_inference/",
	"title": "Bayesian Inference",
	"tags": [],
	"description": "",
	"content": "Chapter 1 Bayesian Inference and MCMC Warning Chapter under construction.\n "
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_6_projects/",
	"title": "Projects",
	"tags": [],
	"description": "",
	"content": "Chapter 1 Personal Projects Warning Chapter under construction.\n "
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_1_linear_algebra/06-cholesky-decomposition/",
	"title": "Cholesky decomposition",
	"tags": [],
	"description": "",
	"content": "Cholesky decomposition Symmetric positive definite matrices are a very special type of matrix that often arise in practice. From a computational point of view, this class of matrix is very attractive because it is possible to decompose a symmetic positive definite matrix $A$ very efficiently into a single lower triangular matrix $G$ so that $A = GG^T$.\nA matrix $A$ is positive definite if $x^T A x \u0026gt; 0$ for any nonzero $x \\in \\mathbb{R}$. This statement by itself is not terribly intuitive, so lets look at also look at an example of a $2 \\times 2$ matrix\n\\[ A = \\left(\\begin{matrix} a_{11} \u0026 a_{12} \\\\ a_{21} \u0026 a_{22} \\end{matrix}\\right) \\]\nIf $A$ is symmetic positive definite (SPD) then\n\\[ \\begin{aligned} x \u0026= (1, 0)^T \\Rightarrow x^T A x = a_{11}  0 \\\\ x \u0026= (0, 1)^T \\Rightarrow x^T A x = a_{22}  0 \\\\ x \u0026= (1, 1)^T \\Rightarrow x^T A x = a_{11} + 2a_{12} + a_{22}  0 \\\\ x \u0026= (1,-1)^T \\Rightarrow x^T A x = a_{11} - 2a_{12} + a_{22}  0 \\\\ \\end{aligned} \\]\nThe first two equations show that the diagonal entries of $A$ must be positive, and combining the last two equations imply $|a_{12}| \\le (a_{11} + a_{22}) / 2$, that is that the matrix has much of its \u0026quot;mass\u0026quot; on the diagonal (note: this is not the same as the matrix being diagonally dominant, where $|a_{ii}| \u0026gt; \\sum_{i=1...n,j \\ne i} |a_{ij}|$). These two observations for our $2 \\times 2$ matrix also apply for a general $n \\times n$ SPD matrix. One of the very nice consequences of this \u0026quot;weighty\u0026quot; diagonal for SPD matrices is that it precludes the need for pivoting.\nIt can be shown that if $A$ is a SPD matrix, then the $LDL^T$ decomposition exists and that $D = \\text{diag}(d_1, ..., d_n)$ has positive diagonal entries. Therefore, it is straightforward to see that $LDL^T$ = $GG^T$, where $G = L \\text{diag}(\\sqrt{d_1}, ..., \\sqrt{d_n})$. The decomposition $A = GG^T$ is known as the cholesky decomposition and can be efficiently constructed in $n^3 / 3$ flops. There are a number of algorithms to construct this decomposition, and both the wikipedia entry and Chapter 4.2 of the Matrix Computations textbook by Golub and Van Loan gives a number of different varients.\nNote that a $LDL$ decomposition can also be used to calculate a cholesky decomposition, and this could be more efficient approach since (a) the SPD structure means that we can neglect pivoting in the $LDL$ decomposition, and (b) the $LDL$ decomposition does not requiring taking the square root of the diagonal elements.\nOther Reading  Golub, G. H. \u0026amp; Van Loan, C. F. Matrix Computations, 3rd Ed. (Johns Hopkins University Press, 1996). Chapter 4.2 https://en.wikipedia.org/wiki/Cholesky_decomposition  Software   scipy.linalg.cholesky  scipy.linalg.cho_factor  scipy.linalg.cho_solve  scipy.linalg.cholesky_banded  scipy.linalg.cho_solve_banded  Problems Imagine that we wanted to sample an array of values $x_i$, for $i = 1...n$, where each value is sampled from an independent Normal distribution with standard deviation $\\sigma$\n\\[x_i \\sim \\mathcal{N}(0, \\sigma)\\]\nThis could be achieved, for example, by sampling from a Normal distribution with unit standard deviation, a function that typically exists in any computer language, then multiplying by $\\sigma$\n\\[x_i = \\sigma \\eta\\]\nwhere $\\eta \\sim \\mathcal{N}(0, 1)$\nNow imagine that instead of an independent Normal distribution you wish to sample $\\mathbf{x} = [x_1, x_2, ..., x_n]$ from a multivariate Normal distribution with some covariance matrix $\\Sigma$\n\\[\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)\\]\nWe can achive this in practice by using the Cholesky decomposition. A covariance matrix is a symmetic positive semidefinite matrix (i.e. $x^T \\Sigma x \\ge 0$}, and therefore can be decomposed into $\\Sigma = LL^T$. We can then draw a sample from $\\mathcal{N}(\\mathbf{0}, \\Sigma)$ by scaling an independently generated random vector by $L$\n\\[\\mathbf{x} = L \\mathbf{\\eta}\\]\nwhere each element of the vector $\\eta$ is $\\eta_i \\sim \\mathcal{N}(0, 1)$.\nWrite Python code to randomly sample an n-dimensional vector $x$ from\n an independent Normal distribution with variance $\\sigma^2$\n a multivariate normal distribution using a covariance matrix $\\Sigma_{ij} = \\exp[(i- j)^2/ \\sigma^2]$\n a multivariate normal distribution with $\\Sigma = \\sigma^2 I$. Show that this algorithm reduces to that used for (1).\n  "
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/06-jacobi-relaxation-methods/",
	"title": "Jacobi and Relaxation Methods",
	"tags": [],
	"description": "",
	"content": "Jacobi Method The Jacobi method is the simplest of the iterative methods, and relies on the fact that the matrix is diagonally dominant. Starting from the problem definition:\n\\[ A\\mathbf{x} = \\mathbf{b} \\]\nwe decompose $A$ in to $A = L + D + U$, where $L$ is lower triagular, $D$ is diagonal, $U$ is upper triangular.\n\\[ A\\mathbf{x} = L\\mathbf{x} + D\\mathbf{x} + U\\mathbf{x} = \\mathbf{b} \\]\nWe then assume that we have an initial guess at the solution $\\mathbf{x}^0$, and try to find a new estimate $\\mathbf{x}^1$. Assuming that the diagonal $D$ dominates over $L$ and $U$, a sensible choice would be to insert $x^0$ and the unknown $x^1$ into the equation like so:\n\\[ L\\mathbf{x}^0 + D\\mathbf{x}^1 + U\\mathbf{x}^0 = \\mathbf{b} \\]\nwe can rearrange to get an equation for $x^1$. This is easily solved as we can take the inverse of the diagonal matrix by simply inverting each diagonal element individually:\n\\[ D\\mathbf{x}_1 = \\mathbf{b} - (L+U)\\mathbf{x}_0 \\]\nThus we end up with the general Jacobi iteration:\n\\[ \\mathbf{x}_{k+1} = D^{-1}(\\mathbf{b} - (L+U)\\mathbf{x}_k) \\]\nRelaxation methods The Jacobi method is an example of a relaxation method, where the matrix $A$ is split into a dominant part $M$ (which is easy to solve), and the remainder $N$. That is, $A = M - N$\n\\(M\\mathbf{x}_{k+1} = N\\mathbf{x}_k + \\mathbf{b}\\) \\(\\mathbf{x}_{k+1} = M^{-1}N\\mathbf{x}_k + M^{-1}\\mathbf{b}\\)\nFor the Jacobi method $M = D$ and $N = -(L + U)$. Other relaxation methods include Gauss-Seidel, where $M = (D + L)$ and $N = -U$, and successive over-relaxation (SOR), where $M = D + \\omega L$ and $N = (1 - \\omega) D - \\omega U$, where $\\omega$ is the relaxation parameter.\nFor any relaxation method to converge we need $\\rho(M^{-1}N) \u0026lt; 1$, where $\\rho()$ is the spectral radius of $M^{-1} N$, which is defined as the largest eigenvalue $\\lambda$ of a a given matrix $G$:\n\\[ \\rho(G) = \\max{|\\lambda|: \\lambda \\in \\lambda(G)} \\]\nFor the SOR method, the relaxation parameter $\\omega$ is generally chosen to minimise $\\rho(M^{-1}N)$, so that the speed of convergence is maximised. In some cases this optimal $\\omega$ is known, for example for finite difference discretisation of the Poisson equation. However, in many cases sophisticated eigenvalue analysis is required to determine the optimal $\\omega$.\nOther Reading  Golub, G. H. \u0026amp; Van Loan, C. F. Matrix Computations, 3rd Ed. (Johns Hopkins University Press, 1996). Chapter 10 Barrett, R., Berry, M., Chan, T. F., Demmel, J., Donato, J., Dongarra, J., ... \u0026amp; Van der Vorst, H. (1994). Templates for the solution of linear systems: building blocks for iterative methods. Society for Industrial and Applied Mathematics.  Problems This exercise involves the manipulation and solution of the linear system resulting from the finite difference solution to Poisson's equation in two dimensions. Let $A$ be a sparse symmetric positive definite matrix of dimension $(N-1)^2 \\times (N-1)^2$ created using scipy.sparse (for a given $N$) by the function buildA as follows:\nimport numpy as np import scipy.sparse as sp def buildA(N): dx = 1 / N nvar = (N - 1)^2; e1 = np.ones((nvar, 1)); e2 = e1 e2[1:N-1:nvar] = 0 e3 = e1 e3[N-1:N-1:nvar] = 0 A = sp.spdiags( np.vstack((-e1, 4*e1, -e1)), -(N-1):N-1:N-1, nvar, nvar ) + sp.spdiags( np.vstack((-e3, -e2), -1:2:1 , nvar, nvar ) A= A / dx^2; and let $\\mathbf{f}_1$ and $\\mathbf{f}_2$ be the vectors defined in buildf1 and buildf2\ndef buildf1(N): x = 0:1/N:1 y = x f = np.dot(np.sin(pi*x), np.sin(pi*y)) return f[2:N,2:N].reshape(-1,1)def buildf2(N): x = 0:1/N:1 y = x f = np.dot(np.max(x,1-x), np.max(y,1-y)) return f[2:N,2:N].reshape(-1, 1) We will consider manipulation of the matrix $A$ and solution of the linear systems $A\\mathbf{U}_i=\\mathbf{f}_i$. The solution to this linear system corresponds to a finite difference solution to Poisson's equation $-\\nabla^2 u = f$ on the unit square with zero Dirichlet boundary conditions where $f$ is either $\\sin(\\pi x) \\sin (\\pi y)$ or $\\max(x,1-x) \\max(y,1-y)$. PDEs of this type occur (usually with some additional reaction and or convection terms) very frequently in mathematical modelling of physiological processes, and even in image analysis.\n Write a function to solve a linear system using the Jacobi method. In terms of $N$, how many iterations does it take to converge? (Try $N=4,8,16,32,64$.) Write a function to solve a linear system using the SOR method. For $N=64$ and right-hand-side $\\mathbf{f}_2$ determine numerically the best choice of the relaxation parameter to 2 decimal places and compare this with theory.  "
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_4_optimisation/06-nelder-mead/",
	"title": "Nelder-Mead method",
	"tags": [],
	"description": "",
	"content": "The Nelder-Mead method is popular and implementations exist in many optimisation software libraries. It is based on the idea of a simplex in parameter space of dimension $n$, which is formed from the convex hull of $n + 1$ points in $\\mathcal{R}^n$. These points $x_i$ are ordered according to their function value so that\n\\[ f(x_1) \\le f(x_2) \\le \\cdots \\le f(x_{n+1}) \\]\nFor each iteration of the algorithm, there are five different points of interest, the first of which is the centroid of the $n$ points with the lowest $f$ values\n\\[ \\bar{x} = \\sum_{i=1}^n x_i \\]\nThe other four points are defined by considering the line joining $\\bar{x}$ and the point with the highest $f$ value $x_{n+1}$\n\\[ \\bar{x}(t) = \\bar{x} + t(x_{n+1} - \\bar{x}) \\]\nThe four points are the reflection, expanding, the outside contraction and inside constraction points, given by $\\bar{x}(-1)$, $\\bar{x}(-2)$, $\\bar{x}(1/2)$, and $\\bar{x}(-1/2)$ respectivly.\nThe Nelder-Mead algorithm tries to replace $x_{n+1}$ by reflecting, expanding, or contracting the simplex to one of these points. If it cannot find a better point, then all the vertices on the simplex are shrunk towards the best vertex $x_1$.\n\nScholarpedia and Wikipedia provide diagrams and pseudocode of the Nelder-Mead algorithm, as does Chapter 9.5 of the Nocedal and Write textbook given below\nOther Reading  Nelder, John A.; R. Mead (1965). \u0026quot;A simplex method for function minimization\u0026quot;. Computer Journal. 7 (4): 308–313. doi:10.1093/comjnl/7.4.308. Numerical optimization by Nocedal, Jorge; Wright, Stephen J., 1960-, Chapter 9  Problems  Code up the Nelder-Mead algorithm and compare its performance against the steepest descent, Newton and dogleg algorithms you did in the last lesson. You can evaluate them on the 2D quadratic function $f(x, y) = x^2 + y^2$, the 2D Rosenbrock function or on one of many different optimisation test functions  "
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_1_linear_algebra/07-qr-decomposition/",
	"title": "QR decomposition",
	"tags": [],
	"description": "",
	"content": "QR decomposition The least-squares problem One of the most important application of the $QR$ decomposition is the least squares solution of a set of overdetermined equations. That is a set of $m$ linear equations with $n$ unknowns, with $m \\ge n$. The least squares problem to be solved is the mimimisation of $||A x - b ||_2$, where $|| x ||_2 = \\sqrt{x_1^2 + x_2^2 + ... + x_m^2}$ is the standard 2-norm, and where $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$ and $b \\in \\mathbb{R}^m$. In this case, the problem $Ax = b$ will often have no solution, and thus it is nessessary to consider $Ax$ and $b$ as approximatelly equal, and to minimise the distance between them by minimising the loss function $||A x - b||_2$.\nTo solve this least squares problem, we need to consider the subspace of all vectors in $\\mathbb{R}^{m}$ that are formed from linear combinations of the columns of $A$. This is known as the column space of the $A$, and is denoted as $\\text{Col }A$. Given that any linear combination of the columns of $A$ will lie in this space, we can say that $Ax$ will also lie in $\\text{Col }A$ for any $x$.\nNow consider a projection of $b$ into the column space of $A$ to give a new vector $\\hat{b}$ (i.e. $\\hat{b}$ is the closest point in Col $A$ to $b$), see the diagram below. Because $\\hat{b}$ is in the column space of $A$, we know that there is another vector $\\hat{x}$ that also lies in Col $A$ and satisfies\n\\[ A \\hat{x} = \\hat{b} \\]\nSince $\\hat{b}$ is the closest point to $b$ in the column space of $A$, we can therefore say that $\\hat{x}$ is the least-squares solution.\n\nWe can show that the vector $b - \\hat{b} = b - A \\hat{x}$ is orthogonal to Col $A$ and therefore also orthogonal to each column in $A$, so we have $a_j^T (b - A \\hat{x})$ for each column $a_j$ of $A$. Putting these $m$ equations together we can write\n\\[ A^T (b - A \\hat{x}) = 0 \\]\nor rearranged slightly, we can find the least-sqaures solution $\\hat{x}$ via the solution of the equation\n\\[ A^T A \\hat{x} = A^T b \\]\nThe $QR$ decomposition divides $A = QR$ into an orthogonal matrix $Q$, and an upper triangular matrix $R$. Most importantly for the least-squares problem, the matrix $Q$ is also an orthonormal basis for Col $A$ and therefore $\\hat{b} = Q Q^T b$.\nGiven this decomposition, it can be shown that the least squares solution of $A x = b$ is given by\n\\[ \\hat{x} = R^{-1} Q^T b \\]\nTo prove this, we let $\\hat{x} = R^{-1} Q^T b$ and make the following substitutions\n\\[ A\\hat{x} = QR \\hat{x} = QRR^{-1}Q^T b = Q Q^T b = \\hat{b} \\]\nTherefore $A\\hat{x} = \\hat{b}$, which proves that $\\hat{x}$ is the least-squares solution for $A x = b$\nFinally, we note that the inverse $R^{-1}$ should not be calculated directly, but instead $\\hat{x}$ should be found by solving\n\\[ R x = Q^T b \\]\nConstructing the QR decomposition $QR$ decomposisions are normally computed via Householder reflections, Givens rotations or the Gram-Schmidt process. For a brief summary of the first two methods, it is useful to consider a simple $2 \\times 2$ reflection or rotation of a 2d vector. For example, the matrix\n\\[ Q = \\left(\\begin{matrix} \\cos(\\theta) \u0026 \\sin(\\theta) \\\\ -\\sin(\\theta) \u0026 \\cos(\\theta) \\end{matrix}\\right) \\]\nis a rotation matrix that when applied to a vector $x$ will result in $y = Qx$, where $y$ is rotated counterclockwise through the angle $\\theta$. $Q$ is also orthogonal since $QQ^T = I$.\nSimilarly, a $2 \\times 2$ reflection matrix can be constructed as\n\\[ Q = \\left(\\begin{matrix} \\cos(\\theta) \u0026 \\sin(\\theta) \\\\ \\sin(\\theta) \u0026 -\\cos(\\theta) \\end{matrix}\\right) \\]\nwhich when applied to a vector $x$ will result in $y = Qx$, where $y$ is reflected across the line defined by $\\text{span}((\\cos(\\theta), \\sin(\\theta))^T)$.\nRotations and reflections are often useful because they can be selected in order to introduce zeros to the vector they are applied to. Given an $m \\times n$ matrix $A$, a series of $n$ Householder reflections can be applied to reduce $A$ to an upper triangular matrix $R$\n\\[ H_n ... H_2 H_1 A = R \\]\nBy setting $Q = H_1 H_2 ... H_n$, we can show that $A = QR$, and that $Q$ is an orthogonal matrix which is also an orthonormal basis for the column space of $A$.\nSimilarly, a Givens rotation can be used to zero a single component of $A$, so that a a series of rotations can be used to contruct the upper triangular matrix $R$\n\\[ G_j ... G_2 G_1 A = R \\]\nso that $Q = G_1 G_2 ... G_j$, and $A = QR$. For both the Householder and Givens methods, it is often useful to not construct the full matrix $Q$ but to keep $Q$ factored as a implicit product of either $H_1 H_2 ... H_n$ or $G_1 G_2 ... G_j$. Fast algorithms exist to calculate the produce of these factored forms to another vector.\nThe final method to contruct a $QR$ decomposition is using the Gram-Schmidt process, which is a process for contructing an orthogonal or orthonormal basis for a given subspace defined by the span of the set of vectors $x_1, x_2, ..., x_n$. If these $n$ vectors are the columns of the $m \\times n$ matrix $A$, then the Gram-Schmidt process can be used to directly contruct the orthonormal basis of the column space of $A$ given by $Q$, and that $A = QR$ where $R$ is an upper triangular matrix. The matrix $R$ can be calculated using $R = Q^T A$. Note that the classical Gram-Schmidt exhibits poor numerical qualities, therefore a modified version of the algorithm exists, which is described in the Golub and Van Loan Matrix Computations textbook listed below.\nIn terms of computational work, the Householder method takes $2n^2(m-n/3)$ flops to compute $Q$ in factored form, and another $2n^2(m-n/3)$ to get the full matrix $Q$, whereas the Gram-Schmidt method is more efficient at $2mn^2$ flops. However, Householder is normally prefered in practice as even with the modified algorithm the numerical properies of the Gram-Schmidt are still poor in comparison with both Householder and Givens (i.e. the final orthogonality of $Q$ is not ideal), so is only useful when the columns of $A$ are already fairly independent. Using Givens rotations the matrix $R$ can be found in $2n^2(m-n/3)$, or the factorised form of the $QR$ decomposition can be found in the same amount of time. The full matrix $Q$ is not normally calculated via Givens rotations. Using Givens rotations is most useful when there are only few non-zeros in $A$, and is more easily parallised than Householder.\nOther Reading The discussion in this section relied on concepts such as orthogonal and orthonormal vector pairs, vector spaces and subspaces and basis vectors. It is well worth investigating these topics further in:\n Linear algebra and its applications by David C. Lay. Chapers 4 \u0026amp; 6.  Additional reading on the $QR$ decomposition can be found at:\n Linear algebra and its applications by David C. Lay. Chaper 6.4\n Golub, G. H. \u0026amp; Van Loan, C. F. Matrix Computations, 3rd Ed. (Johns Hopkins University Press, 1996). Chapter 5.2 https://en.wikipedia.org/wiki/QR_decomposition  Software   scipy.linalg.qr  scipy.linalg.qr_multiply  scipy.linalg.qr_update  scipy.linalg.qr_delete  scipy.linalg.qr_insert  Problems For this exercises we will be using some data on Oxford's weather which is hosted by Saad Jbabdi from the Wellcome Centre for Integrative NeuroImaging (FMRIB), which can be obtained here.\nWe wish to fit a quadratic model of the form $y = a x^2 + b x + c$ to the hours of sunlight observed in Oxford (7th column in OxfordWeather.txt) versus the month (2nd column). The dataset in question has $m \u0026gt; 3$ data points, so our model gives us a set of $m$ equations for 3 unknowns $a$, $b$, and $c$ that are overdetermined, that is, for each data point $(y_i, x_i)$ for $i=1..m$ we have:\n\\[ y_i = a x_i^2 + b x_i + c \\]\nUse a $QR$ decomposition to find the least-squares solution to these equations, and therefore fit the model to the data. Plot the model and the data side by side to qualitativly evaluate the fit.\n"
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/07-conjugate-gradient-method/",
	"title": "Conjugate Gradient Method",
	"tags": [],
	"description": "",
	"content": "One of the most important classes of iterative methods are the Krylov subspace methods, which include:\n Congugate Gradient (CG): for symmetrix positive definite matrices Biconjugate Gradient Stabilized (BiCGSTAB): for general square matrices Generalized Minimal Residual (GMRES): for general square matrices  Below we will give a brief summary of the CG method, for more details you can consult the text by Golub and Van Loan (Chapter 10).\nThe CG method is based on minimising the function\n\\[ \\phi(x) = \\frac{1}{2}x^T A x - x^T b \\]\nIf we set $x$ to the solution of $Ax =b$, that is $x = A^{-1} b$, then the value of $\\phi(x)$ is at its minimum $\\phi(A^{-1} b) = -b^T A^{-1} b / 2$, showing that solving $Ax = b$ and minimising $\\phi$ are equivalent.\nAt each iteration $k$ of CG we are concerned with the residual, defined as $r_k = b - A x_k$. If the residual is nonzero, then at each step we wish to find a positive $\\alpha$ such that $\\phi(x_k + \\alpha p_k) \u0026lt; \\phi(x_k)$, where $p_k$ is the search direction at each $k$. For the classical stepest descent optimisation algorithm the search direction would be the residual $p_k = r_k$, however, steapest descent can suffer from convergence problems, so instead we aim to find a set of search directions $p_k$ so that $p_k^T r_{k-1} \\ne 0$ (i.e. at each step we are guarenteed to reduce $\\phi$), and that the search directions are linearly independent. The latter guarentees that the method will converge in at most $n$ steps, where $n$ is the size of the square matrix $A$.\nIt can be shown that the best set of search directions can be achieved by setting\n\\[ \\begin{aligned} \\beta_k \u0026= \\frac{-p^T_{k-1} A r_{k-1}}{p^T_{k-1} A p_{k-1}} \\\\ p_k \u0026= r_{k-1} + \\beta_k p_{k-1} \\\\ \\alpha_k \u0026= \\frac{p^T_k r_{k-1}}{p^T_k A p_k} \\end{aligned} \\]\nDirectly using the above equations in an iterative algorithm results in the standard CG algorithm. A more efficient algorithm can be derived from this by computing the residuals recursivly via $r_k = r_{k-1} - \\alpha_k A p_k$, leading to the final algorithm given below (reproduced from Wikipedia):\n\nPreconditioning The CG method works well (i.e. converges quickly) if the condition number of the matrix $A$ is low. The condition number of a matrix gives a measure of how much the solution $x$ changes in response to a small change in the input $b$, and is a property of the matrix $A$ itself, so can vary from problem to problem. In order to keep the number of iterations small for iterative solvers, it is therefore often neccessary to use a preconditioner, which is a method of transforming what might be a difficult problem with a poorly conditioned $A$, into a well conditioned problem that is easy to solve.\nConsider the case of precoditioning for the CG methods, we start from the standard problem $A x = b$, and we wish to solve an equivilent transformed problem given by\n\\[ \\tilde{A} \\tilde{x} = \\tilde{b} \\]\nwhere $\\tilde{A} = C^{-1} A C^{-1}$, $\\tilde{x} = Cx$, $\\tilde{b} = C^{-1}$, and $C$ is a symmetric positive matrix.\nWe then simply apply the standard CG method as given above to this transformed problem. This leads to an algorithm which is then simplified by instead computing the transformed quantities $\\tilde{p}_k = C p_k$, $\\tilde{x}_k = C x_k$, and $\\tilde{r}_k = C^{-1} r_k$. Finally we define a matrix $M = C^2$, which is known as the preconditioner, leading to the final precoditioned CG algorithm given below (reproduced and edited from Wikipedia):\n$\\mathbf{r}_0 := \\mathbf{b} - \\mathbf{A x}_0$\n$\\mathbf{z}_0 := \\mathbf{M}^{-1} \\mathbf{r}_0$\n$\\mathbf{p}_0 := \\mathbf{z}_0$\n$k := 0 \\, $\nrepeat until $|| \\mathbf{r}_k ||_2 \u0026lt; \\epsilon ||\\mathbf{b}||_2$\n\u0026nbsp;\u0026nbsp; $\\alpha_k := \\frac{\\mathbf{r}_k^T \\mathbf{z}_k}{ \\mathbf{p}_k^T \\mathbf{A p}_k }$\n\u0026nbsp;\u0026nbsp; $\\mathbf{x}_{k+1} := \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$ \u0026nbsp;\u0026nbsp; $\\mathbf{r}_{k+1} := \\mathbf{r}_k - \\alpha_k \\mathbf{A p}_k$ \u0026nbsp;\u0026nbsp; if $r_{k+1}$ is sufficiently small then exit loop end if \u0026nbsp;\u0026nbsp; $\\mathbf{z}_{k+1} := \\mathbf{M}^{-1} \\mathbf{r}_{k+1}$\n\u0026nbsp;\u0026nbsp; $\\beta_k := \\frac{\\mathbf{r}_{k+1}^T \\mathbf{z}_{k+1}}{\\mathbf{r}_k^T \\mathbf{z}_k}$\n\u0026nbsp;\u0026nbsp; $\\mathbf{p}_{k+1} := \\mathbf{z}_{k+1} + \\beta_k \\mathbf{p}_k$\n\u0026nbsp;\u0026nbsp; $k := k + 1 \\, $\nend repeat\nThe key point to note here is that the preconditioner is used by inverting $M$, so this matrix must be \u0026quot;easy\u0026quot; to solve in some fashion, and also result in a transformed problem with better conditioning.\nTermination: The CG algorithm is normally run untill convergence to a given tolerance which is based on the norm of the input vector $b$. In the algorithm above we iterate until the residual norm is less than some fraction (set by the user) of the norm of $b$.\nWhat preconditioner to choose for a given problem is often highly problem-specific, but some useful general purpose preconditioners exist, such as the incomplete Cholesky preconditioner for preconditioned CG (see Chapter 10.3.2 of the Golub \u0026amp; Van Loan text given below). Chapter 3 of the Barrett et al. text, also cited below, contains descriptions of a few more commonly used preconditioners.\nOther Reading  Golub, G. H. \u0026amp; Van Loan, C. F. Matrix Computations, 3rd Ed. (Johns Hopkins University Press, 1996). Chapter 10 Barrett, R., Berry, M., Chan, T. F., Demmel, J., Donato, J., Dongarra, J., ... \u0026amp; Van der Vorst, H. (1994). Templates for the solution of linear systems: building blocks for iterative methods. Society for Industrial and Applied Mathematics.  "
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/08-scipy.sparse.linalg/",
	"title": "Scipy.sparse.linalg",
	"tags": [],
	"description": "",
	"content": "Once again the best resource for Python is the scipi.sparse.linalg documentation.\nIterative methods for linear equation systems in scipy.sparse.linalg:\n BIConjugate Gradient iteration (BiCG) BIConjugate Gradient STABilized iteration (BiCGSTAB) Conjugate Gradient iteration (CG) Conjugate Gradient Squared iteration (CGS) Generalized Minimal RESidual iteration (GMRES)  LGMRES MINimum RESidual iteration (MINRES) Quasi-Minimal Residual iteration (QMR)  GCROT(m,k)  scipy.sparse.linalg also contains two iterative solvers for least-squares problems, lsqr and lsmr\nProblems Note: based on the Finite Difference matrix $A$ in a previous exercise:\nFor $N=4,8,16,32,64,128$ try the following:\n Solve the linear systems using $\\mathbf{U}_i=A^{-1} \\mathbf{f}_i$ (see scipy.linalg.inv and record the time this takes on a $\\log$-$\\log$ graph. (Omit the case $N=128$ and note this may take a while for $N=64$.) Solve the linear systems using the $\\text{LU}$ and Cholesky decompositions. Plot the time this takes on the same graph. Now solve the systems iteratively using a conjugate gradients solver (you can use the one in scipy.linalg.sparse, or you can code up your own). How many iterations are needed for each problem? Explain the results for the right-hand-side $\\mathbf{f}_1$. For the right-hand-side $\\mathbf{f}_2$ what is the relationship between the number of iterations and $N$. How long do the computations take? Repeat using the scipy.sparse.linalg BICGSTAB and GMRES solvers.  "
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Modelling and Scientific Computing in Python "
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_0_introduction/01_course_overview/",
	"title": "Course Overview",
	"tags": [],
	"description": "",
	"content": "Welcome to \u0026lsquo;Scientific Computing in MATLAB\u0026rsquo;. We are delighted you have chosen this course and hope it will meet your expectations.\nIn this preliminary unit we shall be looking at:\n communications managing your learning activities.  If you are already familiar with online study, you will be able to skip through these sessions quickly and get onto the real stuff. If you are not, it may answer a few of your questions. If you have any remaining questions contact your tutor who will be happy to help.\nIf you have questions relating to the academic content or learning, please contact your course tutor, who will introduce him-or herself to you via email.\nMathematical prerequisites The course is a mathematically based one and we assume that you have studied mathematics to A level or equivalent. You should be familiar with the ideas of vectors, matrices, differentiation and integration along with basic probability and statistics. There will be reminders of the key mathematical ideas where they are used, along with appropriate references, but we will not be explaining everything from scratch.\nRequired reading There is no required reading for this course, which is relatively self-contained. References are given throughout the course where you can find more information on particular topics, many of which are online resources or help files. There are however certain places where no appropriate online sources are available, so we have given references to appropriate books.\nRequired software For this course you will need access to a computer with the MATLAB software installed. Section 0.3 contains details on how to get MATLAB. If you already have access to MATLAB then you can skip this section.\nUnits This course is divided into six units, each of which is designed to take between a few hours and a few days to cover, depending on your prior programming and mathematics experience. The units are as follows:\n Unit 1: Basic introduction to MATLAB Unit 2: Data analysis, image analysis and basic statistics in MATLAB Unit 3: Basic calculus in MATLAB Unit 4: Linear algebra in MATLAB Unit 5: Solving ODEs in MATLAB Unit 6: Software engineering and scientific computing  Each unit consists of:\n an introduction giving you an outline of the topic covered in the unit and its learning objectives information about the topic you are studying, giving you the context within which your reading should make sense a number of activities, some of which are pen and paper exercises, and the majority of which are MATLAB exercises (and both types contain hints and solutions) recommended resources for further exploration.  Learning expectations Depending on how the course you are taking is structured, you may be working full-time on this course, or only for a few hours a week. You will find that a lot of that time is spent working on the exercises in MATLAB and – we hope – discussing the issues with your fellow students. This will help you to share ideas and experiences and learn from others as you progress through the course.\nIn undertaking this course, you are committing yourself to:\n regularly reading the course materials working through all activities, which is especially important as this is the main mechanism for learning on this course completing the final assignment.  "
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_0_introduction/02_course_activities/",
	"title": "Course Activities",
	"tags": [],
	"description": "",
	"content": "The main teaching mechanism on this course is through activities and exercises. Each unit is composed of a number of such activities which will not only introduce the material to you but also enable you to demonstrate your understanding (or to express your lack of understanding – something you should never be afraid to do).\nRegular activities and resources There are several kinds of activity that you will be asked to complete while on this course. Some of these will be ongoing throughout the course, whereas others will relate to the topic of a particular week. Activity types include:\nReading: Doing this lets you explore the content of the course. Usually you will receive guidance about things you should be considering as you do the reading to help you get the most from it.\nIndividual activities: These will help you to consolidate the learning from your reading and further explore the ideas in the course. Much of the work you undertake on your own will feed into your assignments.\nThe activities are broken up into the following two categories:\nWalkthroughs: In each of these, you will be taken through an application of MATLAB. You should type all commands given into the MATLAB command window as you work your way through the activity.\nProblems: You will be given a problem to solve using MATLAB (and, very occasionally, pen and paper). This will require the creation of a figure or the development of an appropriate MATLAB function. Hints and solutions will be given.\nThe work you do for many of the exercises on this course will be directly useful in the assignment, which will therefore be much less onerous if you have completed the activities as you go along.\nCode and Mathematics Code will be written in typewriter font, this for example, and you should type these commands into the MATLAB command window when you see them, to see what they do.\nLonger code blocks will appear like this:\nb=ones(500,1); tic, x1=A\\b; toc tic, x2=B\\b; toc Mathematics will be presented in maths font, i.e. $\\Psi(\\mathbf{r},t)$, and longer equations may appear on on their own line, such as:\n$$i\\hbar\\frac{\\partial}{\\partial t} \\Psi(\\mathbf{r},t) = \\left [ \\frac{-\\hbar^2}{2\\mu}\\nabla^2 + V(\\mathbf{r},t)\\right ] \\Psi(\\mathbf{r},t).$$\n"
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/unit_0_introduction/03_getting_matlab/",
	"title": "Getting MATLAB",
	"tags": [],
	"description": "",
	"content": "A pre-requisite for the course is to have the program MATLAB installed on your computer. You will only need the basic version of MATLAB for this course. As you will discover there are many toolboxes are available which extend the functionality of MATLAB but these are not needed for this course. In this section we will get MATLAB set up on your computer.\nChecking for MATLAB If you are using a departmental computer then MATLAB may already have been installed for you. The following will show you if it is installed or not.\nIf you have the following icon on your desktop (or in your start menu) then you already have MATLAB installed: In Linux, try typing matlab into the command line to see if you have MATLAB installed. If it is installed then the MATLAB Command window will open.\nIf MATLAB is installed on your computer then you can ignore the rest of this section. However, if you don\u0026rsquo;t have MATLAB installed on your machine then contact your College or Department\u0026rsquo;s IT support officers and they will be able to help.\nIf you are using a personal computer for this course then there are a few ways of getting MATLAB.\nDownloading MATLAB from inside Oxford University If you are an Oxford University student then MATLAB is available from IT Services under a university-wide student licence, details of which can be found at the following link:\nhttps://help.it.ox.ac.uk/sls/matlab\nYou will need to use your Single Sign-On to get access to MATLAB.\nUsing MATLAB online It is possible to use MATLAB in your web browser by going to:\nhttps://matlab.mathworks.com/\nThis still requires you to have an account with MathWorks, which you can obtain via the University of Oxford by following the instructions above. While using MATLAB in the browser may be sufficient for this course, it is preferable to have MATLAB installed on the machine you will be using. You will get better performance, have access to more features, have have your files stored locally.\nPurchasing MATLAB directly from MathWorks If you don\u0026rsquo;t have an Oxford University Single Sign-On, and your IT support cannot setup MATLAB for you, then you can also purchase MATLAB directly from MathWorks, at https://www.mathworks.com/.\n"
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://sabs-r3.github.io/scientific-computing/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]