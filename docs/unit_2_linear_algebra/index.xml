<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sparse Matrices and Iterative Solvers on Modelling and Scientific Computing</title>
    <link>https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/</link>
    <description>Recent content in Sparse Matrices and Iterative Solvers on Modelling and Scientific Computing</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <lastBuildDate>Fri, 27 Nov 2020 16:10:31 +0000</lastBuildDate><atom:link href="https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Sparse matrices</title>
      <link>https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/01-sparse-matrices/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/01-sparse-matrices/</guid>
      <description>Why sparse matrices Taking advantage of any special structure in the matrix of interest is always of great importance when designing a linear algebra algorithm/solver. Thus far we have discussed special structures such as symmetric or positive definite matrices, but one of the most common matrix structures in scientific computing is that of a sparse matrix, or a matrix containing many zero elements. Since zeros can be ignored in many computations, for example multiplication or addition, a sparse matrix will specify a special data structure so that only the non-zero elements of the matrix are actually stored and used in computation.</description>
    </item>
    
    <item>
      <title>COOrdinate format</title>
      <link>https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/02-coo-matrix/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/02-coo-matrix/</guid>
      <description>As an example of a sparse matrix format, this section describes one of the sparse formats implemented in Scipy, the The COOrdinate format (COO). This is also known as the &amp;quot;ijv&amp;quot; or &amp;quot;triplet&amp;quot; format, and stores the non-zero elements in three arrays, row, col, and data. The data[i] value is the non-zero entry in row row[i] and column col[i] of the matrix. The advantages of this format are:
 fast format for constructing sparse matrices fast conversions to/from the CSR and CSC formats fast matrix-vector multiplication fast elementwise operations (e.</description>
    </item>
    
    <item>
      <title>Finite Difference Matrix</title>
      <link>https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/03-finite-difference/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/03-finite-difference/</guid>
      <description>Many matrices in scientific computing contain mostly zeros, particularly those arising from the discretistaion of partial differential equations (PDEs). Here we will construct a sparse matrix using scipy.sparse that is derived from the finite difference discretistaion of the Poisson equation. In 1D, Poisson equation is
\[u_{xx} = f(x)\text{ for }0 \le x \le 1\]
The central FD approximation of $u_{xx}$ is:
\[u_{xx} \approx \frac{u(x + h) - 2u(x) + u(x-h)}{h^2}\]</description>
    </item>
    
    <item>
      <title>Scipy.sparse and problems</title>
      <link>https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/04-scipy-sparse/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/04-scipy-sparse/</guid>
      <description>There are seven available sparse matrix types in scipy.sparse:
 csc_matrix: Compressed Sparse Column format csr_matrix: Compressed Sparse Row format bsr_matrix: Block Sparse Row format lil_matrix: List of Lists format dok_matrix: Dictionary of Keys format coo_matrix: COOrdinate format (aka IJV, triplet format) dia_matrix: DIAgonal format  As indicated by the excellent documentation, the dok_matrix or lil_matrix formats are preferable to construct matrices as they support basic slicing and indexing similar to a standard NumPy array.</description>
    </item>
    
    <item>
      <title>Iterative methods</title>
      <link>https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/05-iterative-methods/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/05-iterative-methods/</guid>
      <description>Previously we have discussed direct linear algebra solvers based on decompositions of the original matrix $A$. The amount of computational effort required to achieve these decomposisions is $\mathcal{O}(n^3)$, where $n$ is the number of rows of a square matrix. They are therefore unsuitable for the large, sparse systems of equations that are typically encountered in scientific applications. An alternate class of linear algebra solvers are the iterative methods, which produce a series of approximate solutions $x_k$ to the $A x = b$ problem.</description>
    </item>
    
    <item>
      <title>Jacobi and Relaxation Methods</title>
      <link>https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/06-jacobi-relaxation-methods/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/06-jacobi-relaxation-methods/</guid>
      <description>Jacobi Method The Jacobi method is the simplest of the iterative methods, and relies on the fact that the matrix is diagonally dominant. Starting from the problem definition:
\[ A\mathbf{x} = \mathbf{b} \]
we decompose $A$ in to $A = L + D + U$, where $L$ is lower triangular, $D$ is diagonal, $U$ is upper triangular.
\[ A\mathbf{x} = L\mathbf{x} + D\mathbf{x} + U\mathbf{x} = \mathbf{b} \]
We then assume that we have an initial guess at the solution $\mathbf{x}^0$, and try to find a new estimate $\mathbf{x}^1$.</description>
    </item>
    
    <item>
      <title>Conjugate Gradient Method</title>
      <link>https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/07-conjugate-gradient-method/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/07-conjugate-gradient-method/</guid>
      <description>One of the most important classes of iterative methods are the Krylov subspace methods, which include:
 Congugate Gradient (CG): for symmetrix positive definite matrices Biconjugate Gradient Stabilized (BiCGSTAB): for general square matrices Generalized Minimal Residual (GMRES): for general square matrices  Below we will give a brief summary of the CG method, for more details you can consult the text by Golub and Van Loan (Chapter 10).
The CG method is based on minimising the function</description>
    </item>
    
    <item>
      <title>Iterative solvers in Scipy</title>
      <link>https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/08-scipy.sparse.linalg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://sabs-r3.github.io/scientific-computing/unit_2_linear_algebra/08-scipy.sparse.linalg/</guid>
      <description>Once again the best resource for Python is the scipi.sparse.linalg documentation. The available iterative solvers in Scipy are:
 BIConjugate Gradient iteration (BiCG) BIConjugate Gradient STABilized iteration (BiCGSTAB) Conjugate Gradient iteration (CG) Conjugate Gradient Squared iteration (CGS) Generalized Minimal RESidual iteration (GMRES)  LGMRES MINimum RESidual iteration (MINRES) Quasi-Minimal Residual iteration (QMR)  GCROT(m,k)  scipy.sparse.linalg also contains two iterative solvers for least-squares problems, lsqr and lsmr
Problems Note: based on the Finite Difference matrix $A$ in a previous exercise:</description>
    </item>
    
  </channel>
</rss>
